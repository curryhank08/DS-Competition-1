{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cs scholars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Browser started successfully.\n",
      "ğŸ“„ Crawling page 1...\n",
      "ğŸŒ Navigating to URL: https://scholargps.com/highly-ranked-scholars?year=2022&field=Engineering+and+Computer+Science&discipline=Computer+Science&p=1\n",
      "ğŸ˜´ Sleeping for 12.91 seconds...\n",
      "ğŸ“„ Crawling page 2...\n",
      "ğŸŒ Navigating to URL: https://scholargps.com/highly-ranked-scholars?year=2022&field=Engineering+and+Computer+Science&discipline=Computer+Science&p=2\n",
      "ğŸ˜´ Sleeping for 15.74 seconds...\n",
      "ğŸ“„ Crawling page 3...\n",
      "ğŸŒ Navigating to URL: https://scholargps.com/highly-ranked-scholars?year=2022&field=Engineering+and+Computer+Science&discipline=Computer+Science&p=3\n",
      "ğŸ˜´ Sleeping for 13.38 seconds...\n",
      "ğŸ“„ Crawling page 4...\n",
      "ğŸŒ Navigating to URL: https://scholargps.com/highly-ranked-scholars?year=2022&field=Engineering+and+Computer+Science&discipline=Computer+Science&p=4\n",
      "ğŸ˜´ Sleeping for 8.27 seconds...\n",
      "ğŸ“„ Crawling page 5...\n",
      "ğŸŒ Navigating to URL: https://scholargps.com/highly-ranked-scholars?year=2022&field=Engineering+and+Computer+Science&discipline=Computer+Science&p=5\n",
      "ğŸ˜´ Sleeping for 9.72 seconds...\n",
      "ğŸ“„ Crawling page 6...\n",
      "ğŸŒ Navigating to URL: https://scholargps.com/highly-ranked-scholars?year=2022&field=Engineering+and+Computer+Science&discipline=Computer+Science&p=6\n",
      "ğŸ˜´ Sleeping for 12.86 seconds...\n",
      "ğŸ“„ Crawling page 7...\n",
      "ğŸŒ Navigating to URL: https://scholargps.com/highly-ranked-scholars?year=2022&field=Engineering+and+Computer+Science&discipline=Computer+Science&p=7\n",
      "ğŸ˜´ Sleeping for 19.14 seconds...\n",
      "ğŸ“„ Crawling page 8...\n",
      "ğŸŒ Navigating to URL: https://scholargps.com/highly-ranked-scholars?year=2022&field=Engineering+and+Computer+Science&discipline=Computer+Science&p=8\n",
      "ğŸ˜´ Sleeping for 12.89 seconds...\n",
      "ğŸ“„ Crawling page 9...\n",
      "ğŸŒ Navigating to URL: https://scholargps.com/highly-ranked-scholars?year=2022&field=Engineering+and+Computer+Science&discipline=Computer+Science&p=9\n",
      "ğŸ˜´ Sleeping for 13.98 seconds...\n",
      "ğŸ“„ Crawling page 10...\n",
      "ğŸŒ Navigating to URL: https://scholargps.com/highly-ranked-scholars?year=2022&field=Engineering+and+Computer+Science&discipline=Computer+Science&p=10\n",
      "ğŸ˜´ Sleeping for 16.18 seconds...\n",
      "ğŸ“„ Crawling page 11...\n",
      "ğŸŒ Navigating to URL: https://scholargps.com/highly-ranked-scholars?year=2022&field=Engineering+and+Computer+Science&discipline=Computer+Science&p=11\n",
      "ğŸ˜´ Sleeping for 12.55 seconds...\n",
      "ğŸ“„ Crawling page 12...\n",
      "ğŸŒ Navigating to URL: https://scholargps.com/highly-ranked-scholars?year=2022&field=Engineering+and+Computer+Science&discipline=Computer+Science&p=12\n",
      "ğŸ˜´ Sleeping for 19.41 seconds...\n",
      "ğŸ“„ Crawling page 13...\n",
      "ğŸŒ Navigating to URL: https://scholargps.com/highly-ranked-scholars?year=2022&field=Engineering+and+Computer+Science&discipline=Computer+Science&p=13\n",
      "âš ï¸ Error in extract_scholars_from_page: Message: \n",
      "\n",
      "ğŸ˜´ Sleeping for 10.42 seconds...\n",
      "ğŸ“„ Crawling page 14...\n",
      "ğŸŒ Navigating to URL: https://scholargps.com/highly-ranked-scholars?year=2022&field=Engineering+and+Computer+Science&discipline=Computer+Science&p=14\n",
      "ğŸ˜´ Sleeping for 10.02 seconds...\n",
      "ğŸ“„ Crawling page 15...\n",
      "ğŸŒ Navigating to URL: https://scholargps.com/highly-ranked-scholars?year=2022&field=Engineering+and+Computer+Science&discipline=Computer+Science&p=15\n",
      "ğŸ˜´ Sleeping for 5.89 seconds...\n",
      "ğŸ“„ Crawling page 16...\n",
      "ğŸŒ Navigating to URL: https://scholargps.com/highly-ranked-scholars?year=2022&field=Engineering+and+Computer+Science&discipline=Computer+Science&p=16\n",
      "ğŸ˜´ Sleeping for 13.82 seconds...\n",
      "ğŸ“„ Crawling page 17...\n",
      "ğŸŒ Navigating to URL: https://scholargps.com/highly-ranked-scholars?year=2022&field=Engineering+and+Computer+Science&discipline=Computer+Science&p=17\n",
      "ğŸ˜´ Sleeping for 19.32 seconds...\n",
      "ğŸ“„ Crawling page 18...\n",
      "ğŸŒ Navigating to URL: https://scholargps.com/highly-ranked-scholars?year=2022&field=Engineering+and+Computer+Science&discipline=Computer+Science&p=18\n",
      "ğŸ˜´ Sleeping for 11.13 seconds...\n",
      "ğŸ“„ Crawling page 19...\n",
      "ğŸŒ Navigating to URL: https://scholargps.com/highly-ranked-scholars?year=2022&field=Engineering+and+Computer+Science&discipline=Computer+Science&p=19\n",
      "ğŸ˜´ Sleeping for 10.94 seconds...\n",
      "ğŸ“„ Crawling page 20...\n",
      "ğŸŒ Navigating to URL: https://scholargps.com/highly-ranked-scholars?year=2022&field=Engineering+and+Computer+Science&discipline=Computer+Science&p=20\n",
      "âš ï¸ Error in extract_scholars_from_page: Message: \n",
      "\n",
      "ğŸ˜´ Sleeping for 16.39 seconds...\n",
      "ğŸ“„ Crawling page 21...\n",
      "ğŸŒ Navigating to URL: https://scholargps.com/highly-ranked-scholars?year=2022&field=Engineering+and+Computer+Science&discipline=Computer+Science&p=21\n",
      "âš ï¸ Error in extract_scholars_from_page: Message: \n",
      "\n",
      "ğŸ˜´ Sleeping for 5.68 seconds...\n",
      "ğŸ“„ Crawling page 22...\n",
      "ğŸŒ Navigating to URL: https://scholargps.com/highly-ranked-scholars?year=2022&field=Engineering+and+Computer+Science&discipline=Computer+Science&p=22\n",
      "âš ï¸ Error in extract_scholars_from_page: Message: \n",
      "\n",
      "ğŸ˜´ Sleeping for 17.14 seconds...\n",
      "ğŸ“„ Crawling page 23...\n",
      "ğŸŒ Navigating to URL: https://scholargps.com/highly-ranked-scholars?year=2022&field=Engineering+and+Computer+Science&discipline=Computer+Science&p=23\n",
      "âš ï¸ Error in extract_scholars_from_page: Message: \n",
      "\n",
      "ğŸ˜´ Sleeping for 11.79 seconds...\n",
      "ğŸ“„ Crawling page 24...\n",
      "ğŸŒ Navigating to URL: https://scholargps.com/highly-ranked-scholars?year=2022&field=Engineering+and+Computer+Science&discipline=Computer+Science&p=24\n",
      "âš ï¸ Error in extract_scholars_from_page: Message: \n",
      "\n",
      "ğŸ˜´ Sleeping for 13.02 seconds...\n",
      "ğŸ“„ Crawling page 25...\n",
      "ğŸŒ Navigating to URL: https://scholargps.com/highly-ranked-scholars?year=2022&field=Engineering+and+Computer+Science&discipline=Computer+Science&p=25\n",
      "âš ï¸ Error in extract_scholars_from_page: Message: \n",
      "\n",
      "ğŸ˜´ Sleeping for 8.38 seconds...\n",
      "ğŸ“„ Crawling page 26...\n",
      "ğŸŒ Navigating to URL: https://scholargps.com/highly-ranked-scholars?year=2022&field=Engineering+and+Computer+Science&discipline=Computer+Science&p=26\n",
      "âš ï¸ Error in extract_scholars_from_page: Message: \n",
      "\n",
      "ğŸ˜´ Sleeping for 6.03 seconds...\n",
      "ğŸ“„ Crawling page 27...\n",
      "ğŸŒ Navigating to URL: https://scholargps.com/highly-ranked-scholars?year=2022&field=Engineering+and+Computer+Science&discipline=Computer+Science&p=27\n",
      "âš ï¸ Error in extract_scholars_from_page: Message: \n",
      "\n",
      "ğŸ˜´ Sleeping for 7.20 seconds...\n",
      "ğŸ“„ Crawling page 28...\n",
      "ğŸŒ Navigating to URL: https://scholargps.com/highly-ranked-scholars?year=2022&field=Engineering+and+Computer+Science&discipline=Computer+Science&p=28\n",
      "âš ï¸ Error in extract_scholars_from_page: Message: \n",
      "\n",
      "ğŸ˜´ Sleeping for 14.38 seconds...\n",
      "ğŸ“„ Crawling page 29...\n",
      "ğŸŒ Navigating to URL: https://scholargps.com/highly-ranked-scholars?year=2022&field=Engineering+and+Computer+Science&discipline=Computer+Science&p=29\n",
      "âš ï¸ Error in extract_scholars_from_page: Message: \n",
      "\n",
      "ğŸ˜´ Sleeping for 14.94 seconds...\n",
      "ğŸ“„ Crawling page 30...\n",
      "ğŸŒ Navigating to URL: https://scholargps.com/highly-ranked-scholars?year=2022&field=Engineering+and+Computer+Science&discipline=Computer+Science&p=30\n",
      "âš ï¸ Error in extract_scholars_from_page: Message: \n",
      "\n",
      "ğŸ˜´ Sleeping for 19.87 seconds...\n",
      "ğŸ“„ Crawling page 31...\n",
      "ğŸŒ Navigating to URL: https://scholargps.com/highly-ranked-scholars?year=2022&field=Engineering+and+Computer+Science&discipline=Computer+Science&p=31\n",
      "âš ï¸ Error in extract_scholars_from_page: Message: \n",
      "\n",
      "ğŸ˜´ Sleeping for 14.61 seconds...\n",
      "ğŸ“„ Crawling page 32...\n",
      "ğŸŒ Navigating to URL: https://scholargps.com/highly-ranked-scholars?year=2022&field=Engineering+and+Computer+Science&discipline=Computer+Science&p=32\n",
      "âš ï¸ Error in extract_scholars_from_page: Message: \n",
      "\n",
      "ğŸ˜´ Sleeping for 6.88 seconds...\n",
      "ğŸ“„ Crawling page 33...\n",
      "ğŸŒ Navigating to URL: https://scholargps.com/highly-ranked-scholars?year=2022&field=Engineering+and+Computer+Science&discipline=Computer+Science&p=33\n",
      "âš ï¸ Error in extract_scholars_from_page: Message: \n",
      "\n",
      "ğŸ˜´ Sleeping for 11.26 seconds...\n",
      "ğŸ“„ Crawling page 34...\n",
      "ğŸŒ Navigating to URL: https://scholargps.com/highly-ranked-scholars?year=2022&field=Engineering+and+Computer+Science&discipline=Computer+Science&p=34\n",
      "âš ï¸ Error in extract_scholars_from_page: Message: \n",
      "\n",
      "ğŸ˜´ Sleeping for 9.98 seconds...\n",
      "ğŸ“„ Crawling page 35...\n",
      "ğŸŒ Navigating to URL: https://scholargps.com/highly-ranked-scholars?year=2022&field=Engineering+and+Computer+Science&discipline=Computer+Science&p=35\n",
      "âš ï¸ Error in extract_scholars_from_page: Message: \n",
      "\n",
      "ğŸ˜´ Sleeping for 11.73 seconds...\n",
      "ğŸ“„ Crawling page 36...\n",
      "ğŸŒ Navigating to URL: https://scholargps.com/highly-ranked-scholars?year=2022&field=Engineering+and+Computer+Science&discipline=Computer+Science&p=36\n",
      "âš ï¸ Error in extract_scholars_from_page: Message: \n",
      "\n",
      "ğŸ˜´ Sleeping for 7.23 seconds...\n",
      "ğŸ“„ Crawling page 37...\n",
      "ğŸŒ Navigating to URL: https://scholargps.com/highly-ranked-scholars?year=2022&field=Engineering+and+Computer+Science&discipline=Computer+Science&p=37\n",
      "âš ï¸ Error in extract_scholars_from_page: Message: \n",
      "\n",
      "ğŸ˜´ Sleeping for 8.19 seconds...\n",
      "ğŸ“„ Crawling page 38...\n",
      "ğŸŒ Navigating to URL: https://scholargps.com/highly-ranked-scholars?year=2022&field=Engineering+and+Computer+Science&discipline=Computer+Science&p=38\n",
      "âš ï¸ Error in extract_scholars_from_page: Message: \n",
      "\n",
      "ğŸ˜´ Sleeping for 16.90 seconds...\n",
      "ğŸ“„ Crawling page 39...\n",
      "ğŸŒ Navigating to URL: https://scholargps.com/highly-ranked-scholars?year=2022&field=Engineering+and+Computer+Science&discipline=Computer+Science&p=39\n",
      "âš ï¸ Error in extract_scholars_from_page: Message: \n",
      "\n",
      "ğŸ˜´ Sleeping for 11.95 seconds...\n",
      "ğŸ“„ Crawling page 40...\n",
      "ğŸŒ Navigating to URL: https://scholargps.com/highly-ranked-scholars?year=2022&field=Engineering+and+Computer+Science&discipline=Computer+Science&p=40\n",
      "âš ï¸ Error in extract_scholars_from_page: Message: \n",
      "\n",
      "ğŸ˜´ Sleeping for 5.58 seconds...\n",
      "ğŸ“„ Crawling page 41...\n",
      "ğŸŒ Navigating to URL: https://scholargps.com/highly-ranked-scholars?year=2022&field=Engineering+and+Computer+Science&discipline=Computer+Science&p=41\n",
      "âš ï¸ Error in extract_scholars_from_page: Message: \n",
      "\n",
      "ğŸ˜´ Sleeping for 16.58 seconds...\n",
      "ğŸ“„ Crawling page 42...\n",
      "ğŸŒ Navigating to URL: https://scholargps.com/highly-ranked-scholars?year=2022&field=Engineering+and+Computer+Science&discipline=Computer+Science&p=42\n",
      "âš ï¸ Error in extract_scholars_from_page: Message: \n",
      "\n",
      "ğŸ˜´ Sleeping for 16.30 seconds...\n",
      "ğŸ“„ Crawling page 43...\n",
      "ğŸŒ Navigating to URL: https://scholargps.com/highly-ranked-scholars?year=2022&field=Engineering+and+Computer+Science&discipline=Computer+Science&p=43\n",
      "âš ï¸ Error in extract_scholars_from_page: Message: \n",
      "\n",
      "ğŸ˜´ Sleeping for 15.37 seconds...\n",
      "ğŸ“„ Crawling page 44...\n",
      "ğŸŒ Navigating to URL: https://scholargps.com/highly-ranked-scholars?year=2022&field=Engineering+and+Computer+Science&discipline=Computer+Science&p=44\n",
      "âš ï¸ Error in extract_scholars_from_page: Message: \n",
      "\n",
      "ğŸ˜´ Sleeping for 18.45 seconds...\n",
      "ğŸ“„ Crawling page 45...\n",
      "ğŸŒ Navigating to URL: https://scholargps.com/highly-ranked-scholars?year=2022&field=Engineering+and+Computer+Science&discipline=Computer+Science&p=45\n",
      "âš ï¸ Error in extract_scholars_from_page: Message: \n",
      "\n",
      "ğŸ˜´ Sleeping for 11.99 seconds...\n",
      "ğŸ“„ Crawling page 46...\n",
      "ğŸŒ Navigating to URL: https://scholargps.com/highly-ranked-scholars?year=2022&field=Engineering+and+Computer+Science&discipline=Computer+Science&p=46\n",
      "âš ï¸ Error in extract_scholars_from_page: Message: \n",
      "\n",
      "ğŸ˜´ Sleeping for 9.51 seconds...\n",
      "ğŸ“„ Crawling page 47...\n",
      "ğŸŒ Navigating to URL: https://scholargps.com/highly-ranked-scholars?year=2022&field=Engineering+and+Computer+Science&discipline=Computer+Science&p=47\n",
      "âš ï¸ Error in extract_scholars_from_page: Message: \n",
      "\n",
      "ğŸ˜´ Sleeping for 8.00 seconds...\n",
      "ğŸ“„ Crawling page 48...\n",
      "ğŸŒ Navigating to URL: https://scholargps.com/highly-ranked-scholars?year=2022&field=Engineering+and+Computer+Science&discipline=Computer+Science&p=48\n",
      "âš ï¸ Error in extract_scholars_from_page: Message: \n",
      "\n",
      "ğŸ˜´ Sleeping for 18.25 seconds...\n",
      "ğŸ“„ Crawling page 49...\n",
      "ğŸŒ Navigating to URL: https://scholargps.com/highly-ranked-scholars?year=2022&field=Engineering+and+Computer+Science&discipline=Computer+Science&p=49\n",
      "âš ï¸ Error in extract_scholars_from_page: Message: \n",
      "\n",
      "ğŸ˜´ Sleeping for 19.41 seconds...\n",
      "ğŸ“„ Crawling page 50...\n",
      "ğŸŒ Navigating to URL: https://scholargps.com/highly-ranked-scholars?year=2022&field=Engineering+and+Computer+Science&discipline=Computer+Science&p=50\n",
      "âš ï¸ Error in extract_scholars_from_page: Message: \n",
      "\n",
      "ğŸ˜´ Sleeping for 13.09 seconds...\n",
      "ğŸ›‘ Browser closed.\n",
      "âœ… Data successfully saved to highly_ranked_scholars_2022_3.csv. Total scholars extracted: 346\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "\n",
    "def init_browser():\n",
    "    \"\"\"Initialize the browser with required options and settings.\"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    chrome_options.add_argument(\"--disable-extensions\")\n",
    "    chrome_options.add_argument(\"--disable-infobars\")\n",
    "    chrome_options.add_argument(\"--disable-notifications\")\n",
    "    chrome_options.add_argument(\"--start-maximized\")\n",
    "    chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "\n",
    "    # Remove the comment on this line if you want to run in headless mode\n",
    "    # chrome_options.add_argument(\"--headless\")  \n",
    "\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "    driver.set_page_load_timeout(30)  # Wait up to 30 seconds for page to load\n",
    "\n",
    "    # Remove the \"navigator.webdriver\" property to avoid detection\n",
    "    driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
    "    \n",
    "    print(\"ğŸš€ Browser started successfully.\")\n",
    "    return driver\n",
    "\n",
    "\n",
    "def extract_scholars_from_page(driver, url):\n",
    "    \"\"\"Extract scholars from a single page.\"\"\"\n",
    "    try:\n",
    "        print(f\"ğŸŒ Navigating to URL: {url}\")\n",
    "        driver.get(url)\n",
    "\n",
    "        # Wait until the scholar cards are present on the page\n",
    "        WebDriverWait(driver, 15).until(\n",
    "            EC.presence_of_all_elements_located((By.CLASS_NAME, 'result_box_container'))\n",
    "        )\n",
    "\n",
    "        scholars_data = []\n",
    "\n",
    "        # Find scholar cards on the page\n",
    "        scholar_cards = driver.find_elements(By.CLASS_NAME, 'result_box_container')\n",
    "        \n",
    "        if not scholar_cards:\n",
    "            print(f\"âš ï¸ No scholars found on page: {url}\")\n",
    "        \n",
    "        for card in scholar_cards:\n",
    "            try:\n",
    "                # Extract scholar rank\n",
    "                rank_element = card.find_element(By.CLASS_NAME, 'item1')\n",
    "                rank = rank_element.text.strip().replace('#', '') if rank_element else 'N/A'\n",
    "                \n",
    "                # Extract scholar name\n",
    "                name_element = card.find_element(By.CLASS_NAME, 'scholar_ranking_name')\n",
    "                name = name_element.text.strip() if name_element else 'N/A'\n",
    "                \n",
    "                # Extract scholar institution\n",
    "                institution_element = card.find_element(By.CLASS_NAME, 'fds_rankings_result_container').find_element(By.TAG_NAME, 'a')\n",
    "                institution = institution_element.text.strip() if institution_element else 'N/A'\n",
    "\n",
    "                # Store the scholar's data in a dictionary\n",
    "                scholar_data = {\n",
    "                    'Rank': rank,\n",
    "                    'Name': name,\n",
    "                    'Institution': institution,\n",
    "                }\n",
    "                scholars_data.append(scholar_data)\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Error extracting data for a scholar card: {e}\")\n",
    "                continue\n",
    "\n",
    "        return scholars_data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Error in extract_scholars_from_page: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def crawl_scholargps(base_url, max_pages=100):\n",
    "    \"\"\"Crawl multiple pages of scholar data from ScholarGPS.\"\"\"\n",
    "    driver = init_browser()\n",
    "    all_scholars = []\n",
    "\n",
    "    try:\n",
    "        for page_num in range(1, max_pages + 1):\n",
    "            print(f\"ğŸ“„ Crawling page {page_num}...\")\n",
    "            url = f\"{base_url}&p={page_num}\"\n",
    "            \n",
    "            page_scholars = extract_scholars_from_page(driver, url)\n",
    "            \n",
    "            if page_scholars:\n",
    "                all_scholars.extend(page_scholars)\n",
    "\n",
    "            # Add a human-like delay between page requests\n",
    "            sleep_time = random.uniform(5, 20)\n",
    "            print(f\"ğŸ˜´ Sleeping for {sleep_time:.2f} seconds...\")\n",
    "            time.sleep(sleep_time)\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "        print(\"ğŸ›‘ Browser closed.\")\n",
    "    \n",
    "    return all_scholars\n",
    "\n",
    "\n",
    "# URL for \"Highly Ranked Scholars\" page (2022 for Computer Science)\n",
    "base_url = \"https://scholargps.com/highly-ranked-scholars?year=2022&field=Engineering+and+Computer+Science&discipline=Computer+Science\"\n",
    "\n",
    "# Start crawling for the first 3 pages\n",
    "all_scholars = crawl_scholargps(base_url, max_pages=50)\n",
    "\n",
    "# Save the extracted scholar data to a CSV file\n",
    "df = pd.DataFrame(all_scholars)\n",
    "df.to_csv('highly_ranked_scholars_2022_3.csv', index=False)\n",
    "\n",
    "print(f\"âœ… Data successfully saved to highly_ranked_scholars_2022_3.csv. Total scholars extracted: {len(all_scholars)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the extracted scholar data to a CSV file\n",
    "df = pd.DataFrame(all_scholars)\n",
    "df = df[['Rank', 'Name']].copy()\n",
    "df.to_csv('highly_ranked_scholars_2022_4.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stat scholars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Browser started successfully.\n",
      "ğŸ“„ Crawling page 1...\n",
      "ğŸŒ Navigating to URL: https://scholargps.com/highly-ranked-scholars?year=2022&ranking_duration=LAST_5_YEARS&field=Physical+Sciences+and+Mathematics&discipline=Statistics&p=1\n",
      "ğŸ˜´ Sleeping for 18.86 seconds...\n",
      "ğŸ“„ Crawling page 2...\n",
      "ğŸŒ Navigating to URL: https://scholargps.com/highly-ranked-scholars?year=2022&ranking_duration=LAST_5_YEARS&field=Physical+Sciences+and+Mathematics&discipline=Statistics&p=2\n",
      "ğŸ˜´ Sleeping for 9.79 seconds...\n",
      "ğŸ“„ Crawling page 3...\n",
      "ğŸŒ Navigating to URL: https://scholargps.com/highly-ranked-scholars?year=2022&ranking_duration=LAST_5_YEARS&field=Physical+Sciences+and+Mathematics&discipline=Statistics&p=3\n",
      "ğŸ˜´ Sleeping for 19.33 seconds...\n",
      "ğŸ›‘ Browser closed.\n",
      "âœ… Data successfully saved to highly_ranked_scholars_2022_stat_prior5.csv. Total scholars extracted: 42\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "\n",
    "def init_browser():\n",
    "    \"\"\"Initialize the browser with required options and settings.\"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    chrome_options.add_argument(\"--disable-extensions\")\n",
    "    chrome_options.add_argument(\"--disable-infobars\")\n",
    "    chrome_options.add_argument(\"--disable-notifications\")\n",
    "    chrome_options.add_argument(\"--start-maximized\")\n",
    "    chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "\n",
    "    # Remove the comment on this line if you want to run in headless mode\n",
    "    # chrome_options.add_argument(\"--headless\")  \n",
    "\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "    driver.set_page_load_timeout(30)  # Wait up to 30 seconds for page to load\n",
    "\n",
    "    # Remove the \"navigator.webdriver\" property to avoid detection\n",
    "    driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
    "    \n",
    "    print(\"ğŸš€ Browser started successfully.\")\n",
    "    return driver\n",
    "\n",
    "\n",
    "def extract_scholars_from_page(driver, url):\n",
    "    \"\"\"Extract scholars from a single page.\"\"\"\n",
    "    try:\n",
    "        print(f\"ğŸŒ Navigating to URL: {url}\")\n",
    "        driver.get(url)\n",
    "\n",
    "        # Wait until the scholar cards are present on the page\n",
    "        WebDriverWait(driver, 15).until(\n",
    "            EC.presence_of_all_elements_located((By.CLASS_NAME, 'result_box_container'))\n",
    "        )\n",
    "\n",
    "        scholars_data = []\n",
    "\n",
    "        # Find scholar cards on the page\n",
    "        scholar_cards = driver.find_elements(By.CLASS_NAME, 'result_box_container')\n",
    "        \n",
    "        if not scholar_cards:\n",
    "            print(f\"âš ï¸ No scholars found on page: {url}\")\n",
    "        \n",
    "        for card in scholar_cards:\n",
    "            try:\n",
    "                # Extract scholar rank\n",
    "                rank_element = card.find_element(By.CLASS_NAME, 'item1')\n",
    "                rank = rank_element.text.strip().replace('#', '') if rank_element else 'N/A'\n",
    "                \n",
    "                # Extract scholar name\n",
    "                name_element = card.find_element(By.CLASS_NAME, 'scholar_ranking_name')\n",
    "                name = name_element.text.strip() if name_element else 'N/A'\n",
    "                \n",
    "                # Extract scholar institution\n",
    "                institution_element = card.find_element(By.CLASS_NAME, 'fds_rankings_result_container').find_element(By.TAG_NAME, 'a')\n",
    "                institution = institution_element.text.strip() if institution_element else 'N/A'\n",
    "\n",
    "                # Store the scholar's data in a dictionary\n",
    "                scholar_data = {\n",
    "                    'Rank': rank,\n",
    "                    'Name': name,\n",
    "                    'Institution': institution,\n",
    "                }\n",
    "                scholars_data.append(scholar_data)\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Error extracting data for a scholar card: {e}\")\n",
    "                continue\n",
    "\n",
    "        return scholars_data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Error in extract_scholars_from_page: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def crawl_scholargps(base_url, max_pages=100):\n",
    "    \"\"\"Crawl multiple pages of scholar data from ScholarGPS.\"\"\"\n",
    "    driver = init_browser()\n",
    "    all_scholars = []\n",
    "\n",
    "    try:\n",
    "        for page_num in range(1, max_pages + 1):\n",
    "            print(f\"ğŸ“„ Crawling page {page_num}...\")\n",
    "            url = f\"{base_url}&p={page_num}\"\n",
    "            \n",
    "            page_scholars = extract_scholars_from_page(driver, url)\n",
    "            \n",
    "            if page_scholars:\n",
    "                all_scholars.extend(page_scholars)\n",
    "\n",
    "            # Add a human-like delay between page requests\n",
    "            sleep_time = random.uniform(5, 20)\n",
    "            print(f\"ğŸ˜´ Sleeping for {sleep_time:.2f} seconds...\")\n",
    "            time.sleep(sleep_time)\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "        print(\"ğŸ›‘ Browser closed.\")\n",
    "    \n",
    "    return all_scholars\n",
    "\n",
    "\n",
    "# URL for \"Highly Ranked Scholars\" page (2022 for STAT)\n",
    "base_url = \"https://scholargps.com/highly-ranked-scholars?year=2022&ranking_duration=LAST_5_YEARS&field=Physical+Sciences+and+Mathematics&discipline=Statistics\"\n",
    "\n",
    "# Start crawling for the first 3 pages\n",
    "all_scholars = crawl_scholargps(base_url, max_pages=3)\n",
    "\n",
    "# Save the extracted scholar data to a CSV file\n",
    "df = pd.DataFrame(all_scholars)\n",
    "df = df[['Rank', 'Name']].copy()\n",
    "df.to_csv('highly_ranked_scholars_2022_stat_prior5.csv', index=False)\n",
    "\n",
    "print(f\"âœ… Data successfully saved to highly_ranked_scholars_2022_stat_prior5.csv. Total scholars extracted: {len(all_scholars)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the extracted scholar data to a CSV file\n",
    "df = pd.DataFrame(all_scholars)\n",
    "df = df[['Rank', 'Name']].copy()\n",
    "df.to_csv('highly_ranked_scholars_2022_stat.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ver2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Browser started successfully.\n",
      "ğŸ“„ Crawling page 1...\n",
      "ğŸŒ Navigating to URL: https://scholargps.com/highly-ranked-scholars?year=2022&ranking_duration=LAST_5_YEARS&field=Physical+Sciences+and+Mathematics&discipline=Statistics&p=1\n",
      "ğŸ˜´ Sleeping for 6.95 seconds...\n",
      "ğŸ“„ Crawling page 2...\n",
      "ğŸŒ Navigating to URL: https://scholargps.com/highly-ranked-scholars?year=2022&ranking_duration=LAST_5_YEARS&field=Physical+Sciences+and+Mathematics&discipline=Statistics&p=2\n",
      "ğŸ˜´ Sleeping for 6.40 seconds...\n",
      "ğŸ“„ Crawling page 3...\n",
      "ğŸŒ Navigating to URL: https://scholargps.com/highly-ranked-scholars?year=2022&ranking_duration=LAST_5_YEARS&field=Physical+Sciences+and+Mathematics&discipline=Statistics&p=3\n",
      "ğŸ˜´ Sleeping for 19.47 seconds...\n",
      "ğŸ›‘ Browser closed.\n",
      "âœ… Data successfully saved to highly_ranked_scholars_2022_stat_prior5.csv. Total scholars extracted: 42\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "\n",
    "def init_browser():\n",
    "    \"\"\"Initialize the browser with required options and settings.\"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    chrome_options.add_argument(\"--disable-extensions\")\n",
    "    chrome_options.add_argument(\"--disable-infobars\")\n",
    "    chrome_options.add_argument(\"--disable-notifications\")\n",
    "    chrome_options.add_argument(\"--start-maximized\")\n",
    "    chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "\n",
    "    # Remove the comment on this line if you want to run in headless mode\n",
    "    # chrome_options.add_argument(\"--headless\")  \n",
    "\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "    driver.set_page_load_timeout(30)  # Wait up to 30 seconds for page to load\n",
    "\n",
    "    # Remove the \"navigator.webdriver\" property to avoid detection\n",
    "    driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
    "    \n",
    "    print(\"ğŸš€ Browser started successfully.\")\n",
    "    return driver\n",
    "\n",
    "\n",
    "def extract_scholars_from_page(driver, url):\n",
    "    \"\"\"Extract scholars from a single page.\"\"\"\n",
    "    try:\n",
    "        print(f\"ğŸŒ Navigating to URL: {url}\")\n",
    "        driver.get(url)\n",
    "\n",
    "        # Wait until the scholar cards are present on the page\n",
    "        WebDriverWait(driver, 15).until(\n",
    "            EC.presence_of_all_elements_located((By.CLASS_NAME, 'result_box_container'))\n",
    "        )\n",
    "\n",
    "        scholars_data = []\n",
    "\n",
    "        # Find scholar cards on the page\n",
    "        scholar_cards = driver.find_elements(By.CLASS_NAME, 'result_box_container')\n",
    "        \n",
    "        if not scholar_cards:\n",
    "            print(f\"âš ï¸ No scholars found on page: {url}\")\n",
    "        \n",
    "        for card in scholar_cards:\n",
    "            try:\n",
    "                # Extract scholar rank\n",
    "                rank_element = card.find_element(By.CLASS_NAME, 'item1')\n",
    "                rank = rank_element.text.strip().replace('#', '') if rank_element else 'N/A'\n",
    "                \n",
    "                # Extract scholar name and profile link\n",
    "                name_element = card.find_element(By.CLASS_NAME, 'scholar_ranking_name')\n",
    "                name = name_element.text.strip() if name_element else 'N/A'\n",
    "                profile_link = name_element.get_attribute('href') if name_element else 'N/A'\n",
    "                \n",
    "                # Extract scholar institution\n",
    "                institution_element = card.find_element(By.CLASS_NAME, 'fds_rankings_result_container').find_element(By.TAG_NAME, 'a')\n",
    "                institution = institution_element.text.strip() if institution_element else 'N/A'\n",
    "\n",
    "                # Store the scholar's data in a dictionary\n",
    "                scholar_data = {\n",
    "                    'Rank': rank,\n",
    "                    'Name': name,\n",
    "                    'Profile Link': profile_link,\n",
    "                    'Institution': institution,\n",
    "                }\n",
    "                scholars_data.append(scholar_data)\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Error extracting data for a scholar card: {e}\")\n",
    "                continue\n",
    "\n",
    "        return scholars_data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Error in extract_scholars_from_page: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def crawl_scholargps(base_url, max_pages=100):\n",
    "    \"\"\"Crawl multiple pages of scholar data from ScholarGPS.\"\"\"\n",
    "    driver = init_browser()\n",
    "    all_scholars = []\n",
    "\n",
    "    try:\n",
    "        for page_num in range(1, max_pages + 1):\n",
    "            print(f\"ğŸ“„ Crawling page {page_num}...\")\n",
    "            url = f\"{base_url}&p={page_num}\"\n",
    "            \n",
    "            page_scholars = extract_scholars_from_page(driver, url)\n",
    "            \n",
    "            if page_scholars:\n",
    "                all_scholars.extend(page_scholars)\n",
    "\n",
    "            # Add a human-like delay between page requests\n",
    "            sleep_time = random.uniform(5, 20)\n",
    "            print(f\"ğŸ˜´ Sleeping for {sleep_time:.2f} seconds...\")\n",
    "            time.sleep(sleep_time)\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "        print(\"ğŸ›‘ Browser closed.\")\n",
    "    \n",
    "    return all_scholars\n",
    "\n",
    "\n",
    "# URL for \"Highly Ranked Scholars\" page (2022 for STAT)\n",
    "base_url = \"https://scholargps.com/highly-ranked-scholars?year=2022&ranking_duration=LAST_5_YEARS&field=Physical+Sciences+and+Mathematics&discipline=Statistics\"\n",
    "\n",
    "# Start crawling for the first 3 pages\n",
    "all_scholars = crawl_scholargps(base_url, max_pages=3)\n",
    "\n",
    "# Save the extracted scholar data to a CSV file\n",
    "df = pd.DataFrame(all_scholars)\n",
    "df = df[['Rank', 'Name', 'Profile Link']].copy()\n",
    "df.to_csv('highly_ranked_scholars_2022_stat_prior5.csv', index=False)\n",
    "\n",
    "print(f\"âœ… Data successfully saved to highly_ranked_scholars_2022_stat_prior5.csv. Total scholars extracted: {len(all_scholars)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### publications per scholar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Browser started successfully.\n",
      "ğŸ“„ Extracting publications for Scholar 1/1000: Guiwu Wei\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/78021622765809/guiwu-wei\n",
      "ğŸ˜´ Sleeping for 5.18 seconds...\n",
      "ğŸ“„ Extracting publications for Scholar 2/1000: Mike Thelwall\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/37192170410417/mike-thelwall\n",
      "ğŸ˜´ Sleeping for 10.07 seconds...\n",
      "ğŸ“„ Extracting publications for Scholar 3/1000: Andrew Gelman\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/54874277864582/andrew-gelman\n",
      "ğŸ˜´ Sleeping for 16.08 seconds...\n",
      "ğŸ“„ Extracting publications for Scholar 4/1000: Malin Song\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/54475957797840/malin-song\n",
      "ğŸ˜´ Sleeping for 14.50 seconds...\n",
      "ğŸ“„ Extracting publications for Scholar 5/1000: Daniel Rueckert\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/32739527174271/daniel-rueckert\n",
      "ğŸ˜´ Sleeping for 6.36 seconds...\n",
      "ğŸ“„ Extracting publications for Scholar 6/1000: Jianzhou Wang\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/80604708315804/jianzhou-wang\n",
      "ğŸ˜´ Sleeping for 7.48 seconds...\n",
      "ğŸ“„ Extracting publications for Scholar 7/1000: Muhammad Aslam\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/27977104345336/muhammad-aslam\n",
      "ğŸ˜´ Sleeping for 16.30 seconds...\n",
      "ğŸ“„ Extracting publications for Scholar 8/1000: Deyu Meng\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/19802777203731/deyu-meng\n",
      "ğŸ˜´ Sleeping for 9.42 seconds...\n",
      "ğŸ“„ Extracting publications for Scholar 9/1000: Douglas G. Altman\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/48522964413277/douglas-g-altman\n",
      "ğŸ˜´ Sleeping for 14.43 seconds...\n",
      "ğŸ“„ Extracting publications for Scholar 10/1000: Kyungdo Han\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/74105335250496/kyungdo-han\n",
      "ğŸ˜´ Sleeping for 19.14 seconds...\n",
      "ğŸ“„ Extracting publications for Scholar 11/1000: Gary S. Collins\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/97605358492583/gary-s-collins\n",
      "ğŸ˜´ Sleeping for 6.94 seconds...\n",
      "ğŸ“„ Extracting publications for Scholar 12/1000: Johann S. De Bono\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/22943727653473/johann-s-de-bono\n",
      "ğŸ˜´ Sleeping for 5.56 seconds...\n",
      "ğŸ“„ Extracting publications for Scholar 13/1000: Serge Hercberg\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/13027911474928/serge-hercberg\n",
      "ğŸ˜´ Sleeping for 18.37 seconds...\n",
      "ğŸ“„ Extracting publications for Scholar 14/1000: Trevor Hastie\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/46835416988469/trevor-hastie\n",
      "ğŸ˜´ Sleeping for 19.64 seconds...\n",
      "ğŸ“„ Extracting publications for Scholar 15/1000: Massimo Ciccozzi\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/38368109515868/massimo-ciccozzi\n",
      "ğŸ˜´ Sleeping for 7.82 seconds...\n",
      "ğŸ“„ Extracting publications for Scholar 16/1000: Rob J. Hyndman\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/97330988417379/rob-j-hyndman\n",
      "ğŸ˜´ Sleeping for 10.61 seconds...\n",
      "ğŸ“„ Extracting publications for Scholar 17/1000: Andreas Holzinger\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/63558266544714/andreas-holzinger\n",
      "âš ï¸ Error extracting data for a publication block: Message: no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\".publication_block.authors\"}\n",
      "  (Session info: chrome=128.0.6613.85); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\n",
      "Stacktrace:\n",
      "0   chromedriver                        0x0000000105201208 cxxbridge1$str$ptr + 1927396\n",
      "1   chromedriver                        0x00000001051f966c cxxbridge1$str$ptr + 1895752\n",
      "2   chromedriver                        0x0000000104df4808 cxxbridge1$string$len + 89564\n",
      "3   chromedriver                        0x0000000104e38bcc cxxbridge1$string$len + 369056\n",
      "4   chromedriver                        0x0000000104e2f064 cxxbridge1$string$len + 329272\n",
      "5   chromedriver                        0x0000000104e72228 cxxbridge1$string$len + 604156\n",
      "6   chromedriver                        0x0000000104e2d698 cxxbridge1$string$len + 322668\n",
      "7   chromedriver                        0x0000000104e2e310 cxxbridge1$string$len + 325860\n",
      "8   chromedriver                        0x00000001051c7e78 cxxbridge1$str$ptr + 1693012\n",
      "9   chromedriver                        0x00000001051cc77c cxxbridge1$str$ptr + 1711704\n",
      "10  chromedriver                        0x00000001051ad3ec cxxbridge1$str$ptr + 1583816\n",
      "11  chromedriver                        0x00000001051cd04c cxxbridge1$str$ptr + 1713960\n",
      "12  chromedriver                        0x000000010519dfc8 cxxbridge1$str$ptr + 1521316\n",
      "13  chromedriver                        0x00000001051eab68 cxxbridge1$str$ptr + 1835588\n",
      "14  chromedriver                        0x00000001051eace4 cxxbridge1$str$ptr + 1835968\n",
      "15  chromedriver                        0x00000001051f9308 cxxbridge1$str$ptr + 1894884\n",
      "16  libsystem_pthread.dylib             0x000000018a4832e4 _pthread_start + 136\n",
      "17  libsystem_pthread.dylib             0x000000018a47e0fc thread_start + 8\n",
      "\n",
      "ğŸ˜´ Sleeping for 17.76 seconds...\n",
      "ğŸ“„ Extracting publications for Scholar 18/1000: Peng Ding\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/27359012663364/peng-ding\n",
      "ğŸ˜´ Sleeping for 9.76 seconds...\n",
      "ğŸ“„ Extracting publications for Scholar 19/1000: Yong Du\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/30534071256406/yong-du\n",
      "ğŸ˜´ Sleeping for 12.81 seconds...\n",
      "ğŸ“„ Extracting publications for Scholar 20/1000: Naomi Altman\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/62936355648562/naomi-altman\n",
      "ğŸ˜´ Sleeping for 8.84 seconds...\n",
      "ğŸ“„ Extracting publications for Scholar 21/1000: Giovanni Sotgiu\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/48864668831229/giovanni-sotgiu\n",
      "ğŸ˜´ Sleeping for 15.44 seconds...\n",
      "ğŸ“„ Extracting publications for Scholar 22/1000: Gerta RÃ¼cker\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/46275681792758/gerta-rucker\n",
      "ğŸ˜´ Sleeping for 14.26 seconds...\n",
      "ğŸ“„ Extracting publications for Scholar 23/1000: Martin J. Wainwright\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/41600648351013/martin-j-wainwright\n",
      "ğŸ˜´ Sleeping for 16.81 seconds...\n",
      "ğŸ“„ Extracting publications for Scholar 24/1000: Satya N. Majumdar\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/58885610001161/satya-n-majumdar\n",
      "ğŸ˜´ Sleeping for 8.02 seconds...\n",
      "ğŸ“„ Extracting publications for Scholar 25/1000: Dylan S. Small\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/82363233696669/dylan-s-small\n",
      "ğŸ˜´ Sleeping for 14.64 seconds...\n",
      "ğŸ“„ Extracting publications for Scholar 26/1000: Martin Krzywinski\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/22674494218081/martin-krzywinski\n",
      "ğŸ˜´ Sleeping for 7.60 seconds...\n",
      "ğŸ“„ Extracting publications for Scholar 27/1000: Adam M. Phillippy\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/57082214322872/adam-m-phillippy\n",
      "ğŸ˜´ Sleeping for 8.56 seconds...\n",
      "ğŸ“„ Extracting publications for Scholar 28/1000: Abdul Haq\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/76762668788433/abdul-haq\n",
      "ğŸ˜´ Sleeping for 19.25 seconds...\n",
      "ğŸ“„ Extracting publications for Scholar 29/1000: David B. Dunson\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/47670272104442/david-b-dunson\n",
      "ğŸ˜´ Sleeping for 7.90 seconds...\n",
      "ğŸ“„ Extracting publications for Scholar 30/1000: Stuart J. Pocock\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/21445127017796/stuart-j-pocock\n",
      "ğŸ˜´ Sleeping for 10.90 seconds...\n",
      "ğŸ“„ Extracting publications for Scholar 31/1000: Giovanni Corrao\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/90565966730937/giovanni-corrao\n",
      "ğŸ˜´ Sleeping for 13.08 seconds...\n",
      "ğŸ“„ Extracting publications for Scholar 32/1000: Bin Xu\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/71524100885726/bin-xu\n",
      "ğŸ˜´ Sleeping for 7.53 seconds...\n",
      "ğŸ“„ Extracting publications for Scholar 33/1000: Marcel A. L. M. Van Assen\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/36265512835234/marcel-a-l-m-van-assen\n",
      "ğŸ˜´ Sleeping for 5.08 seconds...\n",
      "ğŸ“„ Extracting publications for Scholar 34/1000: Antonio Gasparrini\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/37158100949600/antonio-gasparrini\n",
      "ğŸ˜´ Sleeping for 7.31 seconds...\n",
      "ğŸ“„ Extracting publications for Scholar 35/1000: Yvan Saeys\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/39773707655258/yvan-saeys\n",
      "ğŸ˜´ Sleeping for 8.84 seconds...\n",
      "ğŸ“„ Extracting publications for Scholar 36/1000: Maxim Finkelstein\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/17738166386261/maxim-finkelstein\n",
      "ğŸ˜´ Sleeping for 19.47 seconds...\n",
      "ğŸ“„ Extracting publications for Scholar 37/1000: Umapada Pal\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/76787955230101/umapada-pal\n",
      "ğŸ˜´ Sleeping for 14.53 seconds...\n",
      "ğŸ“„ Extracting publications for Scholar 38/1000: Anna Chaimani\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/84968385947868/anna-chaimani\n",
      "ğŸ˜´ Sleeping for 13.96 seconds...\n",
      "ğŸ“„ Extracting publications for Scholar 39/1000: JosÃ© Crossa\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/11402201079763/jose-crossa\n",
      "ğŸ˜´ Sleeping for 15.49 seconds...\n",
      "ğŸ“„ Extracting publications for Scholar 40/1000: Sergey Koren\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/15103000048547/sergey-koren\n",
      "ğŸ˜´ Sleeping for 6.91 seconds...\n",
      "ğŸ“„ Extracting publications for Scholar 41/1000: Jiu-Ying Dong\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/69619114487431/jiu-ying-dong\n",
      "ğŸ˜´ Sleeping for 5.34 seconds...\n",
      "ğŸ“„ Extracting publications for Scholar 42/1000: Jun Cheng\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/71957768360152/jun-cheng\n",
      "ğŸ˜´ Sleeping for 14.03 seconds...\n",
      "ğŸ›‘ Browser closed.\n",
      "âœ… Data successfully saved to scholars_publications.csv. Total publications extracted: 839\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "\n",
    "def init_browser():\n",
    "    \"\"\"Initialize the browser with required options and settings.\"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    chrome_options.add_argument(\"--disable-extensions\")\n",
    "    chrome_options.add_argument(\"--disable-infobars\")\n",
    "    chrome_options.add_argument(\"--disable-notifications\")\n",
    "    chrome_options.add_argument(\"--start-maximized\")\n",
    "    chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "\n",
    "    # Remove the comment on this line if you want to run in headless mode\n",
    "    # chrome_options.add_argument(\"--headless\")  \n",
    "\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "    driver.set_page_load_timeout(30)  # Wait up to 30 seconds for page to load\n",
    "\n",
    "    # Remove the \"navigator.webdriver\" property to avoid detection\n",
    "    driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
    "    \n",
    "    print(\"ğŸš€ Browser started successfully.\")\n",
    "    return driver\n",
    "\n",
    "\n",
    "def extract_publications_from_scholar(driver, scholar_profile_url, scholar_name):\n",
    "    \"\"\"Extract publications from a scholar's profile page.\"\"\"\n",
    "    try:\n",
    "        print(f\"ğŸŒ Navigating to Scholar URL: {scholar_profile_url}\")\n",
    "        driver.get(scholar_profile_url)\n",
    "\n",
    "        # Wait until the publication entries are present on the page\n",
    "        WebDriverWait(driver, 15).until(\n",
    "            EC.presence_of_all_elements_located((By.CLASS_NAME, 'result-content'))\n",
    "        )\n",
    "\n",
    "        publications_data = []\n",
    "\n",
    "        # Find publication entries on the scholar's page\n",
    "        publication_blocks = driver.find_elements(By.CLASS_NAME, 'result-content')\n",
    "        \n",
    "        if not publication_blocks:\n",
    "            print(f\"âš ï¸ No publications found on page: {scholar_profile_url}\")\n",
    "        \n",
    "        for block in publication_blocks:\n",
    "            try:\n",
    "                # Extract publication title and link\n",
    "                title_element = block.find_element(By.CLASS_NAME, 'publication_title').find_element(By.TAG_NAME, 'a')\n",
    "                title = title_element.text.strip() if title_element else 'N/A'\n",
    "                publication_link = title_element.get_attribute('href') if title_element else 'N/A'\n",
    "\n",
    "                # Extract publication authors\n",
    "                authors_element = block.find_element(By.CLASS_NAME, 'publication_block.authors')\n",
    "                authors = authors_element.text.strip() if authors_element else 'N/A'\n",
    "                \n",
    "                # Extract publication journal/conference name and year\n",
    "                journal_element = block.find_element(By.CLASS_NAME, 'publication_block.sub-title')\n",
    "                journal_and_year = journal_element.text.strip() if journal_element else 'N/A'\n",
    "\n",
    "                # Extract DOI link\n",
    "                try:\n",
    "                    doi_element = block.find_element(By.CLASS_NAME, 'doi_container').find_element(By.TAG_NAME, 'a')\n",
    "                    doi_link = doi_element.get_attribute('href') if doi_element else 'N/A'\n",
    "                except Exception:\n",
    "                    doi_link = 'N/A'\n",
    "\n",
    "                # Store the publication's data in a dictionary\n",
    "                publication_data = {\n",
    "                    'Scholar Name': scholar_name,\n",
    "                    'Title': title,\n",
    "                    'Publication Link': publication_link,\n",
    "                    'Authors': authors,\n",
    "                    'Journal/Conference': journal_and_year,\n",
    "                    'DOI': doi_link,\n",
    "                    'Scholar Profile': scholar_profile_url\n",
    "                }\n",
    "                publications_data.append(publication_data)\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Error extracting data for a publication block: {e}\")\n",
    "                continue\n",
    "\n",
    "        return publications_data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Error in extract_publications_from_scholar: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def crawl_publications_for_scholars(scholars_data, max_scholars=10):\n",
    "    \"\"\"Crawl publications for multiple scholars using their profile URLs.\"\"\"\n",
    "    driver = init_browser()\n",
    "    all_publications = []\n",
    "\n",
    "    try:\n",
    "        for index, scholar in enumerate(scholars_data[:max_scholars]):\n",
    "            print(f\"ğŸ“„ Extracting publications for Scholar {index + 1}/{max_scholars}: {scholar['Name']}\")\n",
    "            scholar_name = scholar['Name']\n",
    "            scholar_profile_url = scholar['Profile Link']\n",
    "            \n",
    "            scholar_publications = extract_publications_from_scholar(driver, scholar_profile_url, scholar_name)\n",
    "            \n",
    "            if scholar_publications:\n",
    "                all_publications.extend(scholar_publications)\n",
    "\n",
    "            # Add a human-like delay between requests\n",
    "            sleep_time = random.uniform(5, 20)\n",
    "            print(f\"ğŸ˜´ Sleeping for {sleep_time:.2f} seconds...\")\n",
    "            time.sleep(sleep_time)\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "        print(\"ğŸ›‘ Browser closed.\")\n",
    "    \n",
    "    return all_publications\n",
    "\n",
    "link_df = pd.read_csv('highly_ranked_scholars_2022_stat_prior5.csv')\n",
    "scholars_data = []\n",
    "for index in range(len(link_df)):\n",
    "    current_scholar = link_df.iloc[index, :]\n",
    "    current_dic = {}\n",
    "    current_dic['Name'] = current_scholar['Name']\n",
    "    current_dic['Profile Link'] = current_scholar['Profile Link']\n",
    "    scholars_data.append(current_dic)\n",
    "\n",
    "'''\n",
    "# Sample scholar data\n",
    "scholars_data = [\n",
    "    {'Name': 'Guiwu Wei', 'Profile Link': 'https://scholargps.com/scholars/78021622765809/guiwu-wei'},\n",
    "    {'Name': 'Ronald R. Yager', 'Profile Link': 'https://scholargps.com/scholars/86433654059655/ronald-r-yager'},\n",
    "    {'Name': 'Lotfi A. Zadeh', 'Profile Link': 'https://scholargps.com/scholars/29688952374272/lotfi-a-zadeh'},\n",
    "]\n",
    "'''\n",
    "\n",
    "# Crawl the first 3 scholars' publications\n",
    "all_publications = crawl_publications_for_scholars(scholars_data, max_scholars=1000)\n",
    "\n",
    "# Save the extracted publication data to a CSV file\n",
    "df = pd.DataFrame(all_publications)\n",
    "df.to_csv('stat_prior5year_scholars_publications.csv', index=False)\n",
    "\n",
    "print(f\"âœ… Data successfully saved to scholars_publications.csv. Total publications extracted: {len(all_publications)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link_df = pd.read_csv('highly_ranked_scholars_2022_stat_prior5.csv')\n",
    "scholars_data = []\n",
    "for index in range(len(link_df['Name'])):\n",
    "    current_scholar = link_df.iloc[index, :]\n",
    "    current_dic = {}\n",
    "    current_dic['Name'] = current_scholar['Name']\n",
    "    current_dic['Profile Link'] = current_scholar['Profile Link']\n",
    "    scholars_data.append(current_dic)\n",
    "len(scholars_data) == len(link_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### fix page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Browser started successfully.\n",
      "ğŸ“„ Extracting publications for Scholar 1/42: Guiwu Wei\n",
      "ğŸ“„ Extracting publications for Guiwu Wei (Page 1)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/78021622765809/guiwu-wei?p=1\n",
      "ğŸ˜´ Sleeping for 9.49 seconds...\n",
      "ğŸ“„ Extracting publications for Guiwu Wei (Page 2)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/78021622765809/guiwu-wei?p=2\n",
      "ğŸ˜´ Sleeping for 8.95 seconds...\n",
      "ğŸ“„ Extracting publications for Guiwu Wei (Page 3)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/78021622765809/guiwu-wei?p=3\n",
      "ğŸ˜´ Sleeping for 7.75 seconds...\n",
      "ğŸ“„ Extracting publications for Guiwu Wei (Page 4)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/78021622765809/guiwu-wei?p=4\n",
      "ğŸ˜´ Sleeping for 10.61 seconds...\n",
      "ğŸ“„ Extracting publications for Guiwu Wei (Page 5)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/78021622765809/guiwu-wei?p=5\n",
      "ğŸ˜´ Sleeping for 10.56 seconds...\n",
      "ğŸ“„ Extracting publications for Guiwu Wei (Page 6)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/78021622765809/guiwu-wei?p=6\n",
      "ğŸ˜´ Sleeping for 12.46 seconds...\n",
      "ğŸ“„ Extracting publications for Guiwu Wei (Page 7)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/78021622765809/guiwu-wei?p=7\n",
      "ğŸ˜´ Sleeping for 8.18 seconds...\n",
      "ğŸ“„ Extracting publications for Guiwu Wei (Page 8)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/78021622765809/guiwu-wei?p=8\n",
      "ğŸ˜´ Sleeping for 5.17 seconds...\n",
      "ğŸ“„ Extracting publications for Guiwu Wei (Page 9)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/78021622765809/guiwu-wei?p=9\n",
      "ğŸ˜´ Sleeping for 11.61 seconds...\n",
      "ğŸ“„ Extracting publications for Guiwu Wei (Page 10)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/78021622765809/guiwu-wei?p=10\n",
      "ğŸ›‘ Browser closed.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 167\u001b[0m\n\u001b[1;32m    164\u001b[0m     scholars_data\u001b[38;5;241m.\u001b[39mappend(current_dic)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;66;03m# Crawl publications for all listed scholars\u001b[39;00m\n\u001b[0;32m--> 167\u001b[0m all_publications \u001b[38;5;241m=\u001b[39m \u001b[43mcrawl_publications_for_all_scholars\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscholars_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_pages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;66;03m# Save the extracted publication data to a CSV file\u001b[39;00m\n\u001b[1;32m    170\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(all_publications)\n",
      "Cell \u001b[0;32mIn[14], line 138\u001b[0m, in \u001b[0;36mcrawl_publications_for_all_scholars\u001b[0;34m(scholars_data, max_pages)\u001b[0m\n\u001b[1;32m    135\u001b[0m scholar_name \u001b[38;5;241m=\u001b[39m scholar[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mName\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    136\u001b[0m scholar_profile_url \u001b[38;5;241m=\u001b[39m scholar[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProfile Link\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m--> 138\u001b[0m scholar_publications \u001b[38;5;241m=\u001b[39m \u001b[43mcrawl_publications_for_scholar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscholar_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscholar_profile_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_pages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(scholar_publications)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m papers for scholar: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscholar_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m scholar_publications:\n",
      "Cell \u001b[0;32mIn[14], line 110\u001b[0m, in \u001b[0;36mcrawl_publications_for_scholar\u001b[0;34m(driver, scholar_name, scholar_profile_url, max_pages)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m page_num \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, max_pages \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mğŸ“„ Extracting publications for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscholar_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (Page \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage_num\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 110\u001b[0m     page_publications \u001b[38;5;241m=\u001b[39m \u001b[43mextract_publications_from_page\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscholar_profile_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscholar_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpage_num\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m page_publications:\n\u001b[1;32m    113\u001b[0m         all_publications\u001b[38;5;241m.\u001b[39mextend(page_publications)\n",
      "Cell \u001b[0;32mIn[14], line 44\u001b[0m, in \u001b[0;36mextract_publications_from_page\u001b[0;34m(driver, scholar_profile_url, scholar_name, page_number)\u001b[0m\n\u001b[1;32m     42\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscholar_profile_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m?p=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mğŸŒ Navigating to Scholar URL: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 44\u001b[0m \u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Wait until the publication entries are present on the page\u001b[39;00m\n\u001b[1;32m     47\u001b[0m WebDriverWait(driver, \u001b[38;5;241m15\u001b[39m)\u001b[38;5;241m.\u001b[39muntil(\n\u001b[1;32m     48\u001b[0m     EC\u001b[38;5;241m.\u001b[39mpresence_of_all_elements_located((By\u001b[38;5;241m.\u001b[39mCLASS_NAME, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresult-content\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     49\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/scholar_scraper/lib/python3.9/site-packages/selenium/webdriver/remote/webdriver.py:393\u001b[0m, in \u001b[0;36mWebDriver.get\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, url: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    392\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Loads a web page in the current browser session.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 393\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGET\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43murl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/scholar_scraper/lib/python3.9/site-packages/selenium/webdriver/remote/webdriver.py:382\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    379\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msessionId\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[1;32m    380\u001b[0m         params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msessionId\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession_id\n\u001b[0;32m--> 382\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommand_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver_command\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_handler\u001b[38;5;241m.\u001b[39mcheck_response(response)\n",
      "File \u001b[0;32m~/anaconda3/envs/scholar_scraper/lib/python3.9/site-packages/selenium/webdriver/remote/remote_connection.py:404\u001b[0m, in \u001b[0;36mRemoteConnection.execute\u001b[0;34m(self, command, params)\u001b[0m\n\u001b[1;32m    402\u001b[0m trimmed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trim_large_entries(params)\n\u001b[1;32m    403\u001b[0m LOGGER\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, command_info[\u001b[38;5;241m0\u001b[39m], url, \u001b[38;5;28mstr\u001b[39m(trimmed))\n\u001b[0;32m--> 404\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand_info\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/scholar_scraper/lib/python3.9/site-packages/selenium/webdriver/remote/remote_connection.py:428\u001b[0m, in \u001b[0;36mRemoteConnection._request\u001b[0;34m(self, method, url, body)\u001b[0m\n\u001b[1;32m    425\u001b[0m     body \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client_config\u001b[38;5;241m.\u001b[39mkeep_alive:\n\u001b[0;32m--> 428\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    429\u001b[0m     statuscode \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/scholar_scraper/lib/python3.9/site-packages/urllib3/_request_methods.py:143\u001b[0m, in \u001b[0;36mRequestMethods.request\u001b[0;34m(self, method, url, body, fields, headers, json, **urlopen_kw)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_encode_url(\n\u001b[1;32m    136\u001b[0m         method,\n\u001b[1;32m    137\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39murlopen_kw,\n\u001b[1;32m    141\u001b[0m     )\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest_encode_body\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfields\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfields\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43murlopen_kw\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/scholar_scraper/lib/python3.9/site-packages/urllib3/_request_methods.py:278\u001b[0m, in \u001b[0;36mRequestMethods.request_encode_body\u001b[0;34m(self, method, url, fields, headers, encode_multipart, multipart_boundary, **urlopen_kw)\u001b[0m\n\u001b[1;32m    274\u001b[0m     extra_kw[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m\"\u001b[39m, content_type)\n\u001b[1;32m    276\u001b[0m extra_kw\u001b[38;5;241m.\u001b[39mupdate(urlopen_kw)\n\u001b[0;32m--> 278\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mextra_kw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/scholar_scraper/lib/python3.9/site-packages/urllib3/poolmanager.py:443\u001b[0m, in \u001b[0;36mPoolManager.urlopen\u001b[0;34m(self, method, url, redirect, **kw)\u001b[0m\n\u001b[1;32m    441\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 443\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest_uri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m redirect_location \u001b[38;5;241m=\u001b[39m redirect \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mget_redirect_location()\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m redirect_location:\n",
      "File \u001b[0;32m~/anaconda3/envs/scholar_scraper/lib/python3.9/site-packages/urllib3/connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    786\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    805\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/scholar_scraper/lib/python3.9/site-packages/urllib3/connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 536\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m~/anaconda3/envs/scholar_scraper/lib/python3.9/site-packages/urllib3/connection.py:507\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[1;32m    506\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 507\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[0;32m~/anaconda3/envs/scholar_scraper/lib/python3.9/http/client.py:1377\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1375\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1376\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1377\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1378\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1379\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/envs/scholar_scraper/lib/python3.9/http/client.py:320\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    322\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/scholar_scraper/lib/python3.9/http/client.py:281\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 281\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/scholar_scraper/lib/python3.9/socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 704\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    706\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "\n",
    "def init_browser():\n",
    "    \"\"\"Initialize the browser with required options and settings.\"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    chrome_options.add_argument(\"--disable-extensions\")\n",
    "    chrome_options.add_argument(\"--disable-infobars\")\n",
    "    chrome_options.add_argument(\"--disable-notifications\")\n",
    "    chrome_options.add_argument(\"--start-maximized\")\n",
    "    chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "\n",
    "    # Uncomment this line if you want to run in headless mode\n",
    "    # chrome_options.add_argument(\"--headless\")  \n",
    "\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "    driver.set_page_load_timeout(30)  # Wait up to 30 seconds for page to load\n",
    "\n",
    "    # Remove the \"navigator.webdriver\" property to avoid detection\n",
    "    driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
    "    \n",
    "    print(\"ğŸš€ Browser started successfully.\")\n",
    "    return driver\n",
    "\n",
    "\n",
    "def extract_publications_from_page(driver, scholar_profile_url, scholar_name, page_number):\n",
    "    \"\"\"Extract publications from a specific page of a scholar's profile.\"\"\"\n",
    "    try:\n",
    "        url = f\"{scholar_profile_url}?p={page_number}\"\n",
    "        print(f\"ğŸŒ Navigating to Scholar URL: {url}\")\n",
    "        driver.get(url)\n",
    "\n",
    "        # Wait until the publication entries are present on the page\n",
    "        WebDriverWait(driver, 15).until(\n",
    "            EC.presence_of_all_elements_located((By.CLASS_NAME, 'result-content'))\n",
    "        )\n",
    "\n",
    "        publications_data = []\n",
    "\n",
    "        # Find publication entries on the scholar's page\n",
    "        publication_blocks = driver.find_elements(By.CLASS_NAME, 'result-content')\n",
    "        \n",
    "        if not publication_blocks:\n",
    "            print(f\"âš ï¸ No publications found on page: {url}\")\n",
    "        \n",
    "        for block in publication_blocks:\n",
    "            try:\n",
    "                # Extract publication title and link\n",
    "                title_element = block.find_element(By.CLASS_NAME, 'publication_title').find_element(By.TAG_NAME, 'a')\n",
    "                title = title_element.text.strip() if title_element else 'N/A'\n",
    "                publication_link = title_element.get_attribute('href') if title_element else 'N/A'\n",
    "\n",
    "                # Extract publication authors\n",
    "                authors_element = block.find_element(By.CLASS_NAME, 'publication_block.authors')\n",
    "                authors = authors_element.text.strip() if authors_element else 'N/A'\n",
    "                \n",
    "                # Extract publication journal/conference name and year\n",
    "                journal_element = block.find_element(By.CLASS_NAME, 'publication_block.sub-title')\n",
    "                journal_and_year = journal_element.text.strip() if journal_element else 'N/A'\n",
    "\n",
    "                # Extract DOI link\n",
    "                try:\n",
    "                    doi_element = block.find_element(By.CLASS_NAME, 'doi_container').find_element(By.TAG_NAME, 'a')\n",
    "                    doi_link = doi_element.get_attribute('href') if doi_element else 'N/A'\n",
    "                except Exception:\n",
    "                    doi_link = 'N/A'\n",
    "                    \n",
    "                # Extract publication journal/conference name and year\n",
    "                citation_element = block.find_element(By.CLASS_NAME, 'publication_block.source')\n",
    "                citation_count = citation_element.text.strip() if citation_element else 'N/A'\n",
    "\n",
    "                # Store the publication's data in a dictionary\n",
    "                publication_data = {\n",
    "                    'Scholar Name': scholar_name,\n",
    "                    'Title': title,\n",
    "                    'Publication Link': publication_link,\n",
    "                    'Authors': authors,\n",
    "                    'Journal/Conference': journal_and_year,\n",
    "                    'DOI': doi_link,\n",
    "                    'Scholar Profile': scholar_profile_url,\n",
    "                    'Page': page_number,\n",
    "                    'Citation': citation_count\n",
    "                }\n",
    "                publications_data.append(publication_data)\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Error extracting data for a publication block: {e}\")\n",
    "                continue\n",
    "\n",
    "        return publications_data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Error in extract_publications_from_page: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def crawl_publications_for_scholar(driver, scholar_name, scholar_profile_url, max_pages=5):\n",
    "    \"\"\"Crawl all pages of publications for a single scholar.\"\"\"\n",
    "    all_publications = []\n",
    "    for page_num in range(1, max_pages + 1):\n",
    "        print(f\"ğŸ“„ Extracting publications for {scholar_name} (Page {page_num})...\")\n",
    "        \n",
    "        page_publications = extract_publications_from_page(driver, scholar_profile_url, scholar_name, page_num)\n",
    "        \n",
    "        if page_publications:\n",
    "            all_publications.extend(page_publications)\n",
    "        else:\n",
    "            # If no publications are found on this page, it's likely the last page\n",
    "            print(f\"ğŸš« No more publications found for {scholar_name} on page {page_num}. Stopping.\")\n",
    "            break\n",
    "\n",
    "        # Add a human-like delay between requests\n",
    "        sleep_time = random.uniform(5, 15)\n",
    "        print(f\"ğŸ˜´ Sleeping for {sleep_time:.2f} seconds...\")\n",
    "        time.sleep(sleep_time)\n",
    "    \n",
    "    return all_publications\n",
    "\n",
    "\n",
    "def crawl_publications_for_all_scholars(scholars_data, max_pages=5):\n",
    "    \"\"\"Crawl publications for multiple scholars using their profile URLs.\"\"\"\n",
    "    driver = init_browser()\n",
    "    all_publications = []\n",
    "\n",
    "    try:\n",
    "        for index, scholar in enumerate(scholars_data):\n",
    "            print(f\"ğŸ“„ Extracting publications for Scholar {index + 1}/{len(scholars_data)}: {scholar['Name']}\")\n",
    "            scholar_name = scholar['Name']\n",
    "            scholar_profile_url = scholar['Profile Link']\n",
    "            \n",
    "            scholar_publications = crawl_publications_for_scholar(driver, scholar_name, scholar_profile_url, max_pages)\n",
    "            \n",
    "            print(f\"Found {len(scholar_publications)} papers for scholar: {scholar_name}\")\n",
    "            \n",
    "            if scholar_publications:\n",
    "                all_publications.extend(scholar_publications)\n",
    "\n",
    "            # Add a human-like delay between scholars\n",
    "            sleep_time = random.uniform(10, 30)\n",
    "            print(f\"ğŸ˜´ Sleeping for {sleep_time:.2f} seconds before next scholar...\")\n",
    "            time.sleep(sleep_time)\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "        print(\"ğŸ›‘ Browser closed.\")\n",
    "    \n",
    "    return all_publications\n",
    "\n",
    "\n",
    "link_df = pd.read_csv('highly_ranked_scholars_2022_stat_prior5.csv')\n",
    "scholars_data = []\n",
    "for index in range(len(link_df)):\n",
    "    current_scholar = link_df.iloc[index, :]\n",
    "    current_dic = {}\n",
    "    current_dic['Name'] = current_scholar['Name']\n",
    "    current_dic['Profile Link'] = current_scholar['Profile Link']\n",
    "    scholars_data.append(current_dic)\n",
    "\n",
    "# Crawl publications for all listed scholars\n",
    "all_publications = crawl_publications_for_all_scholars(scholars_data, max_pages=20)\n",
    "\n",
    "# Save the extracted publication data to a CSV file\n",
    "df = pd.DataFrame(all_publications)\n",
    "df.to_csv('stat_prior5year_scholars_publications_2.csv', index=False)\n",
    "\n",
    "print(f\"âœ… Data successfully saved to stat_prior5year_scholars_publications_2.csv. Total publications extracted: {len(all_publications)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### ver2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Browser started successfully.\n",
      "ğŸ“„ Extracting publications for Scholar 1/42: Guiwu Wei\n",
      "ğŸ“„ Extracting publications for Guiwu Wei (Page 1)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/78021622765809/guiwu-wei?p=1\n",
      "ğŸ˜´ Sleeping for 8.04 seconds...\n",
      "ğŸ“„ Extracting publications for Guiwu Wei (Page 2)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/78021622765809/guiwu-wei?p=2\n",
      "ğŸ˜´ Sleeping for 5.35 seconds...\n",
      "ğŸ“„ Extracting publications for Guiwu Wei (Page 3)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/78021622765809/guiwu-wei?p=3\n",
      "ğŸ˜´ Sleeping for 14.42 seconds...\n",
      "ğŸ“„ Extracting publications for Guiwu Wei (Page 4)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/78021622765809/guiwu-wei?p=4\n",
      "ğŸ˜´ Sleeping for 6.63 seconds...\n",
      "ğŸ“„ Extracting publications for Guiwu Wei (Page 5)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/78021622765809/guiwu-wei?p=5\n",
      "ğŸ˜´ Sleeping for 11.86 seconds...\n",
      "ğŸ“„ Extracting publications for Guiwu Wei (Page 6)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/78021622765809/guiwu-wei?p=6\n",
      "ğŸ˜´ Sleeping for 12.83 seconds...\n",
      "ğŸ“„ Extracting publications for Guiwu Wei (Page 7)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/78021622765809/guiwu-wei?p=7\n",
      "ğŸ˜´ Sleeping for 5.71 seconds...\n",
      "ğŸ“„ Extracting publications for Guiwu Wei (Page 8)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/78021622765809/guiwu-wei?p=8\n",
      "ğŸ˜´ Sleeping for 14.99 seconds...\n",
      "ğŸ“„ Extracting publications for Guiwu Wei (Page 9)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/78021622765809/guiwu-wei?p=9\n",
      "ğŸ˜´ Sleeping for 6.25 seconds...\n",
      "ğŸ“„ Extracting publications for Guiwu Wei (Page 10)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/78021622765809/guiwu-wei?p=10\n",
      "ğŸ˜´ Sleeping for 11.49 seconds...\n",
      "ğŸ“„ Extracting publications for Guiwu Wei (Page 11)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/78021622765809/guiwu-wei?p=11\n",
      "ğŸ˜´ Sleeping for 12.36 seconds...\n",
      "ğŸ“„ Extracting publications for Guiwu Wei (Page 12)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/78021622765809/guiwu-wei?p=12\n",
      "ğŸ˜´ Sleeping for 10.91 seconds...\n",
      "ğŸ“„ Extracting publications for Guiwu Wei (Page 13)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/78021622765809/guiwu-wei?p=13\n",
      "ğŸ˜´ Sleeping for 10.19 seconds...\n",
      "ğŸ“„ Extracting publications for Guiwu Wei (Page 14)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/78021622765809/guiwu-wei?p=14\n",
      "ğŸ˜´ Sleeping for 10.19 seconds...\n",
      "ğŸ“„ Extracting publications for Guiwu Wei (Page 15)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/78021622765809/guiwu-wei?p=15\n",
      "âš ï¸ Error in extract_publications_from_page: Message: \n",
      "\n",
      "ğŸš« No more publications found for Guiwu Wei on page 15. Stopping.\n",
      "ğŸ˜´ Sleeping for 26.25 seconds before next scholar...\n",
      "ğŸ“„ Extracting publications for Scholar 2/42: Mike Thelwall\n",
      "ğŸ“„ Extracting publications for Mike Thelwall (Page 1)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/37192170410417/mike-thelwall?p=1\n",
      "ğŸ˜´ Sleeping for 5.08 seconds...\n",
      "ğŸ“„ Extracting publications for Mike Thelwall (Page 2)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/37192170410417/mike-thelwall?p=2\n",
      "ğŸ˜´ Sleeping for 6.32 seconds...\n",
      "ğŸ“„ Extracting publications for Mike Thelwall (Page 3)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/37192170410417/mike-thelwall?p=3\n",
      "ğŸ˜´ Sleeping for 7.29 seconds...\n",
      "ğŸ“„ Extracting publications for Mike Thelwall (Page 4)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/37192170410417/mike-thelwall?p=4\n",
      "ğŸ˜´ Sleeping for 11.66 seconds...\n",
      "ğŸ“„ Extracting publications for Mike Thelwall (Page 5)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/37192170410417/mike-thelwall?p=5\n",
      "ğŸ˜´ Sleeping for 13.32 seconds...\n",
      "ğŸ“„ Extracting publications for Mike Thelwall (Page 6)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/37192170410417/mike-thelwall?p=6\n",
      "ğŸ˜´ Sleeping for 5.77 seconds...\n",
      "ğŸ“„ Extracting publications for Mike Thelwall (Page 7)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/37192170410417/mike-thelwall?p=7\n",
      "ğŸ˜´ Sleeping for 5.33 seconds...\n",
      "ğŸ“„ Extracting publications for Mike Thelwall (Page 8)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/37192170410417/mike-thelwall?p=8\n",
      "ğŸ˜´ Sleeping for 8.22 seconds...\n",
      "ğŸ“„ Extracting publications for Mike Thelwall (Page 9)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/37192170410417/mike-thelwall?p=9\n",
      "ğŸ˜´ Sleeping for 6.86 seconds...\n",
      "ğŸ“„ Extracting publications for Mike Thelwall (Page 10)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/37192170410417/mike-thelwall?p=10\n",
      "ğŸ˜´ Sleeping for 7.98 seconds...\n",
      "ğŸ“„ Extracting publications for Mike Thelwall (Page 11)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/37192170410417/mike-thelwall?p=11\n",
      "ğŸ˜´ Sleeping for 8.35 seconds...\n",
      "ğŸ“„ Extracting publications for Mike Thelwall (Page 12)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/37192170410417/mike-thelwall?p=12\n",
      "ğŸ˜´ Sleeping for 12.56 seconds...\n",
      "ğŸ“„ Extracting publications for Mike Thelwall (Page 13)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/37192170410417/mike-thelwall?p=13\n",
      "ğŸ˜´ Sleeping for 7.96 seconds...\n",
      "ğŸ“„ Extracting publications for Mike Thelwall (Page 14)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/37192170410417/mike-thelwall?p=14\n",
      "ğŸ˜´ Sleeping for 7.72 seconds...\n",
      "ğŸ“„ Extracting publications for Mike Thelwall (Page 15)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/37192170410417/mike-thelwall?p=15\n",
      "ğŸ˜´ Sleeping for 6.06 seconds...\n",
      "ğŸ“„ Extracting publications for Mike Thelwall (Page 16)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/37192170410417/mike-thelwall?p=16\n",
      "ğŸ˜´ Sleeping for 11.36 seconds...\n",
      "ğŸ“„ Extracting publications for Mike Thelwall (Page 17)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/37192170410417/mike-thelwall?p=17\n",
      "ğŸ˜´ Sleeping for 8.44 seconds...\n",
      "ğŸ“„ Extracting publications for Mike Thelwall (Page 18)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/37192170410417/mike-thelwall?p=18\n",
      "ğŸ˜´ Sleeping for 14.34 seconds...\n",
      "ğŸ“„ Extracting publications for Mike Thelwall (Page 19)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/37192170410417/mike-thelwall?p=19\n",
      "ğŸ˜´ Sleeping for 14.59 seconds...\n",
      "ğŸ“„ Extracting publications for Mike Thelwall (Page 20)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/37192170410417/mike-thelwall?p=20\n",
      "ğŸ˜´ Sleeping for 7.67 seconds...\n",
      "ğŸ˜´ Sleeping for 18.32 seconds before next scholar...\n",
      "ğŸ“„ Extracting publications for Scholar 3/42: Andrew Gelman\n",
      "ğŸ“„ Extracting publications for Andrew Gelman (Page 1)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/54874277864582/andrew-gelman?p=1\n",
      "ğŸ˜´ Sleeping for 13.75 seconds...\n",
      "ğŸ“„ Extracting publications for Andrew Gelman (Page 2)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/54874277864582/andrew-gelman?p=2\n",
      "ğŸ˜´ Sleeping for 10.63 seconds...\n",
      "ğŸ“„ Extracting publications for Andrew Gelman (Page 3)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/54874277864582/andrew-gelman?p=3\n",
      "ğŸ˜´ Sleeping for 6.75 seconds...\n",
      "ğŸ“„ Extracting publications for Andrew Gelman (Page 4)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/54874277864582/andrew-gelman?p=4\n",
      "ğŸ˜´ Sleeping for 9.12 seconds...\n",
      "ğŸ“„ Extracting publications for Andrew Gelman (Page 5)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/54874277864582/andrew-gelman?p=5\n",
      "ğŸ˜´ Sleeping for 11.33 seconds...\n",
      "ğŸ“„ Extracting publications for Andrew Gelman (Page 6)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/54874277864582/andrew-gelman?p=6\n",
      "ğŸ˜´ Sleeping for 11.82 seconds...\n",
      "ğŸ“„ Extracting publications for Andrew Gelman (Page 7)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/54874277864582/andrew-gelman?p=7\n",
      "ğŸ˜´ Sleeping for 8.55 seconds...\n",
      "ğŸ“„ Extracting publications for Andrew Gelman (Page 8)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/54874277864582/andrew-gelman?p=8\n",
      "ğŸ˜´ Sleeping for 5.05 seconds...\n",
      "ğŸ“„ Extracting publications for Andrew Gelman (Page 9)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/54874277864582/andrew-gelman?p=9\n",
      "ğŸ˜´ Sleeping for 7.75 seconds...\n",
      "ğŸ“„ Extracting publications for Andrew Gelman (Page 10)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/54874277864582/andrew-gelman?p=10\n",
      "âš ï¸ Error extracting data for a publication block: Message: no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\".publication_title\"}\n",
      "  (Session info: chrome=128.0.6613.85); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\n",
      "Stacktrace:\n",
      "0   chromedriver                        0x0000000102535208 cxxbridge1$str$ptr + 1927396\n",
      "1   chromedriver                        0x000000010252d66c cxxbridge1$str$ptr + 1895752\n",
      "2   chromedriver                        0x0000000102128808 cxxbridge1$string$len + 89564\n",
      "3   chromedriver                        0x000000010216cbcc cxxbridge1$string$len + 369056\n",
      "4   chromedriver                        0x0000000102163064 cxxbridge1$string$len + 329272\n",
      "5   chromedriver                        0x00000001021a6228 cxxbridge1$string$len + 604156\n",
      "6   chromedriver                        0x0000000102161698 cxxbridge1$string$len + 322668\n",
      "7   chromedriver                        0x0000000102162310 cxxbridge1$string$len + 325860\n",
      "8   chromedriver                        0x00000001024fbe78 cxxbridge1$str$ptr + 1693012\n",
      "9   chromedriver                        0x000000010250077c cxxbridge1$str$ptr + 1711704\n",
      "10  chromedriver                        0x00000001024e13ec cxxbridge1$str$ptr + 1583816\n",
      "11  chromedriver                        0x000000010250104c cxxbridge1$str$ptr + 1713960\n",
      "12  chromedriver                        0x00000001024d1fc8 cxxbridge1$str$ptr + 1521316\n",
      "13  chromedriver                        0x000000010251eb68 cxxbridge1$str$ptr + 1835588\n",
      "14  chromedriver                        0x000000010251ece4 cxxbridge1$str$ptr + 1835968\n",
      "15  chromedriver                        0x000000010252d308 cxxbridge1$str$ptr + 1894884\n",
      "16  libsystem_pthread.dylib             0x00000001823132e4 _pthread_start + 136\n",
      "17  libsystem_pthread.dylib             0x000000018230e0fc thread_start + 8\n",
      "\n",
      "ğŸ˜´ Sleeping for 12.68 seconds...\n",
      "ğŸ“„ Extracting publications for Andrew Gelman (Page 11)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/54874277864582/andrew-gelman?p=11\n",
      "ğŸ˜´ Sleeping for 11.08 seconds...\n",
      "ğŸ“„ Extracting publications for Andrew Gelman (Page 12)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/54874277864582/andrew-gelman?p=12\n",
      "ğŸ˜´ Sleeping for 14.06 seconds...\n",
      "ğŸ“„ Extracting publications for Andrew Gelman (Page 13)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/54874277864582/andrew-gelman?p=13\n",
      "ğŸ˜´ Sleeping for 5.11 seconds...\n",
      "ğŸ“„ Extracting publications for Andrew Gelman (Page 14)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/54874277864582/andrew-gelman?p=14\n",
      "ğŸ˜´ Sleeping for 7.99 seconds...\n",
      "ğŸ“„ Extracting publications for Andrew Gelman (Page 15)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/54874277864582/andrew-gelman?p=15\n",
      "ğŸ˜´ Sleeping for 9.13 seconds...\n",
      "ğŸ“„ Extracting publications for Andrew Gelman (Page 16)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/54874277864582/andrew-gelman?p=16\n",
      "ğŸ˜´ Sleeping for 12.90 seconds...\n",
      "ğŸ“„ Extracting publications for Andrew Gelman (Page 17)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/54874277864582/andrew-gelman?p=17\n",
      "ğŸ˜´ Sleeping for 10.49 seconds...\n",
      "ğŸ“„ Extracting publications for Andrew Gelman (Page 18)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/54874277864582/andrew-gelman?p=18\n",
      "âš ï¸ Error extracting data for a publication block: Message: no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\".publication_block.sub-title\"}\n",
      "  (Session info: chrome=128.0.6613.85); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\n",
      "Stacktrace:\n",
      "0   chromedriver                        0x0000000102535208 cxxbridge1$str$ptr + 1927396\n",
      "1   chromedriver                        0x000000010252d66c cxxbridge1$str$ptr + 1895752\n",
      "2   chromedriver                        0x0000000102128808 cxxbridge1$string$len + 89564\n",
      "3   chromedriver                        0x000000010216cbcc cxxbridge1$string$len + 369056\n",
      "4   chromedriver                        0x0000000102163064 cxxbridge1$string$len + 329272\n",
      "5   chromedriver                        0x00000001021a6228 cxxbridge1$string$len + 604156\n",
      "6   chromedriver                        0x0000000102161698 cxxbridge1$string$len + 322668\n",
      "7   chromedriver                        0x0000000102162310 cxxbridge1$string$len + 325860\n",
      "8   chromedriver                        0x00000001024fbe78 cxxbridge1$str$ptr + 1693012\n",
      "9   chromedriver                        0x000000010250077c cxxbridge1$str$ptr + 1711704\n",
      "10  chromedriver                        0x00000001024e13ec cxxbridge1$str$ptr + 1583816\n",
      "11  chromedriver                        0x000000010250104c cxxbridge1$str$ptr + 1713960\n",
      "12  chromedriver                        0x00000001024d1fc8 cxxbridge1$str$ptr + 1521316\n",
      "13  chromedriver                        0x000000010251eb68 cxxbridge1$str$ptr + 1835588\n",
      "14  chromedriver                        0x000000010251ece4 cxxbridge1$str$ptr + 1835968\n",
      "15  chromedriver                        0x000000010252d308 cxxbridge1$str$ptr + 1894884\n",
      "16  libsystem_pthread.dylib             0x00000001823132e4 _pthread_start + 136\n",
      "17  libsystem_pthread.dylib             0x000000018230e0fc thread_start + 8\n",
      "\n",
      "ğŸ˜´ Sleeping for 11.71 seconds...\n",
      "ğŸ“„ Extracting publications for Andrew Gelman (Page 19)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/54874277864582/andrew-gelman?p=19\n",
      "ğŸ˜´ Sleeping for 12.88 seconds...\n",
      "ğŸ“„ Extracting publications for Andrew Gelman (Page 20)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/54874277864582/andrew-gelman?p=20\n",
      "âš ï¸ Error extracting data for a publication block: Message: no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\".publication_title\"}\n",
      "  (Session info: chrome=128.0.6613.85); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\n",
      "Stacktrace:\n",
      "0   chromedriver                        0x0000000102535208 cxxbridge1$str$ptr + 1927396\n",
      "1   chromedriver                        0x000000010252d66c cxxbridge1$str$ptr + 1895752\n",
      "2   chromedriver                        0x0000000102128808 cxxbridge1$string$len + 89564\n",
      "3   chromedriver                        0x000000010216cbcc cxxbridge1$string$len + 369056\n",
      "4   chromedriver                        0x0000000102163064 cxxbridge1$string$len + 329272\n",
      "5   chromedriver                        0x00000001021a6228 cxxbridge1$string$len + 604156\n",
      "6   chromedriver                        0x0000000102161698 cxxbridge1$string$len + 322668\n",
      "7   chromedriver                        0x0000000102162310 cxxbridge1$string$len + 325860\n",
      "8   chromedriver                        0x00000001024fbe78 cxxbridge1$str$ptr + 1693012\n",
      "9   chromedriver                        0x000000010250077c cxxbridge1$str$ptr + 1711704\n",
      "10  chromedriver                        0x00000001024e13ec cxxbridge1$str$ptr + 1583816\n",
      "11  chromedriver                        0x000000010250104c cxxbridge1$str$ptr + 1713960\n",
      "12  chromedriver                        0x00000001024d1fc8 cxxbridge1$str$ptr + 1521316\n",
      "13  chromedriver                        0x000000010251eb68 cxxbridge1$str$ptr + 1835588\n",
      "14  chromedriver                        0x000000010251ece4 cxxbridge1$str$ptr + 1835968\n",
      "15  chromedriver                        0x000000010252d308 cxxbridge1$str$ptr + 1894884\n",
      "16  libsystem_pthread.dylib             0x00000001823132e4 _pthread_start + 136\n",
      "17  libsystem_pthread.dylib             0x000000018230e0fc thread_start + 8\n",
      "\n",
      "âš ï¸ Error extracting data for a publication block: Message: no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\".publication_title\"}\n",
      "  (Session info: chrome=128.0.6613.85); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\n",
      "Stacktrace:\n",
      "0   chromedriver                        0x0000000102535208 cxxbridge1$str$ptr + 1927396\n",
      "1   chromedriver                        0x000000010252d66c cxxbridge1$str$ptr + 1895752\n",
      "2   chromedriver                        0x0000000102128808 cxxbridge1$string$len + 89564\n",
      "3   chromedriver                        0x000000010216cbcc cxxbridge1$string$len + 369056\n",
      "4   chromedriver                        0x0000000102163064 cxxbridge1$string$len + 329272\n",
      "5   chromedriver                        0x00000001021a6228 cxxbridge1$string$len + 604156\n",
      "6   chromedriver                        0x0000000102161698 cxxbridge1$string$len + 322668\n",
      "7   chromedriver                        0x0000000102162310 cxxbridge1$string$len + 325860\n",
      "8   chromedriver                        0x00000001024fbe78 cxxbridge1$str$ptr + 1693012\n",
      "9   chromedriver                        0x000000010250077c cxxbridge1$str$ptr + 1711704\n",
      "10  chromedriver                        0x00000001024e13ec cxxbridge1$str$ptr + 1583816\n",
      "11  chromedriver                        0x000000010250104c cxxbridge1$str$ptr + 1713960\n",
      "12  chromedriver                        0x00000001024d1fc8 cxxbridge1$str$ptr + 1521316\n",
      "13  chromedriver                        0x000000010251eb68 cxxbridge1$str$ptr + 1835588\n",
      "14  chromedriver                        0x000000010251ece4 cxxbridge1$str$ptr + 1835968\n",
      "15  chromedriver                        0x000000010252d308 cxxbridge1$str$ptr + 1894884\n",
      "16  libsystem_pthread.dylib             0x00000001823132e4 _pthread_start + 136\n",
      "17  libsystem_pthread.dylib             0x000000018230e0fc thread_start + 8\n",
      "\n",
      "ğŸ˜´ Sleeping for 6.36 seconds...\n",
      "ğŸ˜´ Sleeping for 26.32 seconds before next scholar...\n",
      "ğŸ“„ Extracting publications for Scholar 4/42: Malin Song\n",
      "ğŸ“„ Extracting publications for Malin Song (Page 1)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/54475957797840/malin-song?p=1\n",
      "ğŸ˜´ Sleeping for 13.99 seconds...\n",
      "ğŸ“„ Extracting publications for Malin Song (Page 2)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/54475957797840/malin-song?p=2\n",
      "ğŸ˜´ Sleeping for 8.34 seconds...\n",
      "ğŸ“„ Extracting publications for Malin Song (Page 3)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/54475957797840/malin-song?p=3\n",
      "ğŸ˜´ Sleeping for 8.78 seconds...\n",
      "ğŸ“„ Extracting publications for Malin Song (Page 4)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/54475957797840/malin-song?p=4\n",
      "âš ï¸ Error in extract_publications_from_page: Message: \n",
      "\n",
      "ğŸš« No more publications found for Malin Song on page 4. Stopping.\n",
      "ğŸ˜´ Sleeping for 13.04 seconds before next scholar...\n",
      "ğŸ“„ Extracting publications for Scholar 5/42: Daniel Rueckert\n",
      "ğŸ“„ Extracting publications for Daniel Rueckert (Page 1)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/32739527174271/daniel-rueckert?p=1\n",
      "âš ï¸ Error in extract_publications_from_page: Message: \n",
      "\n",
      "ğŸš« No more publications found for Daniel Rueckert on page 1. Stopping.\n",
      "ğŸ˜´ Sleeping for 10.55 seconds before next scholar...\n",
      "ğŸ“„ Extracting publications for Scholar 6/42: Jianzhou Wang\n",
      "ğŸ“„ Extracting publications for Jianzhou Wang (Page 1)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/80604708315804/jianzhou-wang?p=1\n",
      "âš ï¸ Error in extract_publications_from_page: Message: \n",
      "\n",
      "ğŸš« No more publications found for Jianzhou Wang on page 1. Stopping.\n",
      "ğŸ˜´ Sleeping for 22.96 seconds before next scholar...\n",
      "ğŸ“„ Extracting publications for Scholar 7/42: Muhammad Aslam\n",
      "ğŸ“„ Extracting publications for Muhammad Aslam (Page 1)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/27977104345336/muhammad-aslam?p=1\n",
      "ğŸ˜´ Sleeping for 7.92 seconds...\n",
      "ğŸ“„ Extracting publications for Muhammad Aslam (Page 2)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/27977104345336/muhammad-aslam?p=2\n",
      "ğŸ˜´ Sleeping for 7.31 seconds...\n",
      "ğŸ“„ Extracting publications for Muhammad Aslam (Page 3)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/27977104345336/muhammad-aslam?p=3\n",
      "ğŸ˜´ Sleeping for 7.86 seconds...\n",
      "ğŸ“„ Extracting publications for Muhammad Aslam (Page 4)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/27977104345336/muhammad-aslam?p=4\n",
      "ğŸ˜´ Sleeping for 9.26 seconds...\n",
      "ğŸ“„ Extracting publications for Muhammad Aslam (Page 5)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/27977104345336/muhammad-aslam?p=5\n",
      "ğŸ˜´ Sleeping for 13.21 seconds...\n",
      "ğŸ“„ Extracting publications for Muhammad Aslam (Page 6)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/27977104345336/muhammad-aslam?p=6\n",
      "ğŸ˜´ Sleeping for 10.34 seconds...\n",
      "ğŸ“„ Extracting publications for Muhammad Aslam (Page 7)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/27977104345336/muhammad-aslam?p=7\n",
      "ğŸ˜´ Sleeping for 12.04 seconds...\n",
      "ğŸ“„ Extracting publications for Muhammad Aslam (Page 8)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/27977104345336/muhammad-aslam?p=8\n",
      "ğŸ˜´ Sleeping for 12.86 seconds...\n",
      "ğŸ“„ Extracting publications for Muhammad Aslam (Page 9)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/27977104345336/muhammad-aslam?p=9\n",
      "ğŸ˜´ Sleeping for 5.71 seconds...\n",
      "ğŸ“„ Extracting publications for Muhammad Aslam (Page 10)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/27977104345336/muhammad-aslam?p=10\n",
      "ğŸ˜´ Sleeping for 11.74 seconds...\n",
      "ğŸ“„ Extracting publications for Muhammad Aslam (Page 11)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/27977104345336/muhammad-aslam?p=11\n",
      "ğŸ˜´ Sleeping for 10.36 seconds...\n",
      "ğŸ“„ Extracting publications for Muhammad Aslam (Page 12)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/27977104345336/muhammad-aslam?p=12\n",
      "ğŸ˜´ Sleeping for 8.15 seconds...\n",
      "ğŸ“„ Extracting publications for Muhammad Aslam (Page 13)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/27977104345336/muhammad-aslam?p=13\n",
      "ğŸ˜´ Sleeping for 6.94 seconds...\n",
      "ğŸ“„ Extracting publications for Muhammad Aslam (Page 14)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/27977104345336/muhammad-aslam?p=14\n",
      "ğŸ˜´ Sleeping for 13.79 seconds...\n",
      "ğŸ“„ Extracting publications for Muhammad Aslam (Page 15)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/27977104345336/muhammad-aslam?p=15\n",
      "ğŸ˜´ Sleeping for 7.16 seconds...\n",
      "ğŸ“„ Extracting publications for Muhammad Aslam (Page 16)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/27977104345336/muhammad-aslam?p=16\n",
      "ğŸ˜´ Sleeping for 13.26 seconds...\n",
      "ğŸ“„ Extracting publications for Muhammad Aslam (Page 17)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/27977104345336/muhammad-aslam?p=17\n",
      "ğŸ˜´ Sleeping for 9.81 seconds...\n",
      "ğŸ“„ Extracting publications for Muhammad Aslam (Page 18)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/27977104345336/muhammad-aslam?p=18\n",
      "ğŸ˜´ Sleeping for 7.26 seconds...\n",
      "ğŸ“„ Extracting publications for Muhammad Aslam (Page 19)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/27977104345336/muhammad-aslam?p=19\n",
      "ğŸ˜´ Sleeping for 10.13 seconds...\n",
      "ğŸ“„ Extracting publications for Muhammad Aslam (Page 20)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/27977104345336/muhammad-aslam?p=20\n",
      "ğŸ˜´ Sleeping for 13.83 seconds...\n",
      "ğŸ˜´ Sleeping for 12.16 seconds before next scholar...\n",
      "ğŸ“„ Extracting publications for Scholar 8/42: Deyu Meng\n",
      "ğŸ“„ Extracting publications for Deyu Meng (Page 1)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/19802777203731/deyu-meng?p=1\n",
      "ğŸ˜´ Sleeping for 13.99 seconds...\n",
      "ğŸ“„ Extracting publications for Deyu Meng (Page 2)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/19802777203731/deyu-meng?p=2\n",
      "ğŸ˜´ Sleeping for 7.75 seconds...\n",
      "ğŸ“„ Extracting publications for Deyu Meng (Page 3)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/19802777203731/deyu-meng?p=3\n",
      "ğŸ˜´ Sleeping for 8.70 seconds...\n",
      "ğŸ“„ Extracting publications for Deyu Meng (Page 4)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/19802777203731/deyu-meng?p=4\n",
      "ğŸ˜´ Sleeping for 7.92 seconds...\n",
      "ğŸ“„ Extracting publications for Deyu Meng (Page 5)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/19802777203731/deyu-meng?p=5\n",
      "ğŸ˜´ Sleeping for 11.98 seconds...\n",
      "ğŸ“„ Extracting publications for Deyu Meng (Page 6)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/19802777203731/deyu-meng?p=6\n",
      "ğŸ˜´ Sleeping for 8.56 seconds...\n",
      "ğŸ“„ Extracting publications for Deyu Meng (Page 7)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/19802777203731/deyu-meng?p=7\n",
      "ğŸ˜´ Sleeping for 9.31 seconds...\n",
      "ğŸ“„ Extracting publications for Deyu Meng (Page 8)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/19802777203731/deyu-meng?p=8\n",
      "ğŸ˜´ Sleeping for 12.63 seconds...\n",
      "ğŸ“„ Extracting publications for Deyu Meng (Page 9)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/19802777203731/deyu-meng?p=9\n",
      "ğŸ˜´ Sleeping for 10.06 seconds...\n",
      "ğŸ“„ Extracting publications for Deyu Meng (Page 10)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/19802777203731/deyu-meng?p=10\n",
      "ğŸ˜´ Sleeping for 7.55 seconds...\n",
      "ğŸ“„ Extracting publications for Deyu Meng (Page 11)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/19802777203731/deyu-meng?p=11\n",
      "ğŸ˜´ Sleeping for 14.92 seconds...\n",
      "ğŸ“„ Extracting publications for Deyu Meng (Page 12)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/19802777203731/deyu-meng?p=12\n",
      "ğŸ˜´ Sleeping for 14.72 seconds...\n",
      "ğŸ“„ Extracting publications for Deyu Meng (Page 13)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/19802777203731/deyu-meng?p=13\n",
      "âš ï¸ Error in extract_publications_from_page: Message: \n",
      "\n",
      "ğŸš« No more publications found for Deyu Meng on page 13. Stopping.\n",
      "ğŸ˜´ Sleeping for 16.25 seconds before next scholar...\n",
      "ğŸ“„ Extracting publications for Scholar 9/42: Douglas G. Altman\n",
      "ğŸ“„ Extracting publications for Douglas G. Altman (Page 1)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/48522964413277/douglas-g-altman?p=1\n",
      "ğŸ˜´ Sleeping for 13.18 seconds...\n",
      "ğŸ“„ Extracting publications for Douglas G. Altman (Page 2)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/48522964413277/douglas-g-altman?p=2\n",
      "ğŸ˜´ Sleeping for 7.57 seconds...\n",
      "ğŸ“„ Extracting publications for Douglas G. Altman (Page 3)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/48522964413277/douglas-g-altman?p=3\n",
      "ğŸ˜´ Sleeping for 6.97 seconds...\n",
      "ğŸ“„ Extracting publications for Douglas G. Altman (Page 4)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/48522964413277/douglas-g-altman?p=4\n",
      "ğŸ˜´ Sleeping for 9.54 seconds...\n",
      "ğŸ“„ Extracting publications for Douglas G. Altman (Page 5)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/48522964413277/douglas-g-altman?p=5\n",
      "ğŸ˜´ Sleeping for 10.97 seconds...\n",
      "ğŸ“„ Extracting publications for Douglas G. Altman (Page 6)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/48522964413277/douglas-g-altman?p=6\n",
      "ğŸ˜´ Sleeping for 13.01 seconds...\n",
      "ğŸ“„ Extracting publications for Douglas G. Altman (Page 7)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/48522964413277/douglas-g-altman?p=7\n",
      "ğŸ˜´ Sleeping for 9.04 seconds...\n",
      "ğŸ“„ Extracting publications for Douglas G. Altman (Page 8)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/48522964413277/douglas-g-altman?p=8\n",
      "ğŸ˜´ Sleeping for 7.49 seconds...\n",
      "ğŸ“„ Extracting publications for Douglas G. Altman (Page 9)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/48522964413277/douglas-g-altman?p=9\n",
      "ğŸ˜´ Sleeping for 13.10 seconds...\n",
      "ğŸ“„ Extracting publications for Douglas G. Altman (Page 10)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/48522964413277/douglas-g-altman?p=10\n",
      "ğŸ˜´ Sleeping for 14.56 seconds...\n",
      "ğŸ“„ Extracting publications for Douglas G. Altman (Page 11)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/48522964413277/douglas-g-altman?p=11\n",
      "ğŸ˜´ Sleeping for 12.40 seconds...\n",
      "ğŸ“„ Extracting publications for Douglas G. Altman (Page 12)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/48522964413277/douglas-g-altman?p=12\n",
      "âš ï¸ Error extracting data for a publication block: Message: no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\".publication_title\"}\n",
      "  (Session info: chrome=128.0.6613.85); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\n",
      "Stacktrace:\n",
      "0   chromedriver                        0x0000000102535208 cxxbridge1$str$ptr + 1927396\n",
      "1   chromedriver                        0x000000010252d66c cxxbridge1$str$ptr + 1895752\n",
      "2   chromedriver                        0x0000000102128808 cxxbridge1$string$len + 89564\n",
      "3   chromedriver                        0x000000010216cbcc cxxbridge1$string$len + 369056\n",
      "4   chromedriver                        0x0000000102163064 cxxbridge1$string$len + 329272\n",
      "5   chromedriver                        0x00000001021a6228 cxxbridge1$string$len + 604156\n",
      "6   chromedriver                        0x0000000102161698 cxxbridge1$string$len + 322668\n",
      "7   chromedriver                        0x0000000102162310 cxxbridge1$string$len + 325860\n",
      "8   chromedriver                        0x00000001024fbe78 cxxbridge1$str$ptr + 1693012\n",
      "9   chromedriver                        0x000000010250077c cxxbridge1$str$ptr + 1711704\n",
      "10  chromedriver                        0x00000001024e13ec cxxbridge1$str$ptr + 1583816\n",
      "11  chromedriver                        0x000000010250104c cxxbridge1$str$ptr + 1713960\n",
      "12  chromedriver                        0x00000001024d1fc8 cxxbridge1$str$ptr + 1521316\n",
      "13  chromedriver                        0x000000010251eb68 cxxbridge1$str$ptr + 1835588\n",
      "14  chromedriver                        0x000000010251ece4 cxxbridge1$str$ptr + 1835968\n",
      "15  chromedriver                        0x000000010252d308 cxxbridge1$str$ptr + 1894884\n",
      "16  libsystem_pthread.dylib             0x00000001823132e4 _pthread_start + 136\n",
      "17  libsystem_pthread.dylib             0x000000018230e0fc thread_start + 8\n",
      "\n",
      "ğŸ˜´ Sleeping for 9.64 seconds...\n",
      "ğŸ“„ Extracting publications for Douglas G. Altman (Page 13)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/48522964413277/douglas-g-altman?p=13\n",
      "ğŸ˜´ Sleeping for 14.28 seconds...\n",
      "ğŸ“„ Extracting publications for Douglas G. Altman (Page 14)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/48522964413277/douglas-g-altman?p=14\n",
      "ğŸ˜´ Sleeping for 9.74 seconds...\n",
      "ğŸ“„ Extracting publications for Douglas G. Altman (Page 15)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/48522964413277/douglas-g-altman?p=15\n",
      "ğŸ˜´ Sleeping for 13.48 seconds...\n",
      "ğŸ“„ Extracting publications for Douglas G. Altman (Page 16)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/48522964413277/douglas-g-altman?p=16\n",
      "ğŸ˜´ Sleeping for 14.43 seconds...\n",
      "ğŸ“„ Extracting publications for Douglas G. Altman (Page 17)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/48522964413277/douglas-g-altman?p=17\n",
      "ğŸ˜´ Sleeping for 6.73 seconds...\n",
      "ğŸ“„ Extracting publications for Douglas G. Altman (Page 18)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/48522964413277/douglas-g-altman?p=18\n",
      "ğŸ˜´ Sleeping for 9.37 seconds...\n",
      "ğŸ“„ Extracting publications for Douglas G. Altman (Page 19)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/48522964413277/douglas-g-altman?p=19\n",
      "ğŸ˜´ Sleeping for 6.31 seconds...\n",
      "ğŸ“„ Extracting publications for Douglas G. Altman (Page 20)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/48522964413277/douglas-g-altman?p=20\n",
      "ğŸ˜´ Sleeping for 5.36 seconds...\n",
      "ğŸ˜´ Sleeping for 10.44 seconds before next scholar...\n",
      "ğŸ“„ Extracting publications for Scholar 10/42: Kyungdo Han\n",
      "ğŸ“„ Extracting publications for Kyungdo Han (Page 1)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/74105335250496/kyungdo-han?p=1\n",
      "ğŸ˜´ Sleeping for 7.53 seconds...\n",
      "ğŸ“„ Extracting publications for Kyungdo Han (Page 2)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/74105335250496/kyungdo-han?p=2\n",
      "ğŸ˜´ Sleeping for 12.26 seconds...\n",
      "ğŸ“„ Extracting publications for Kyungdo Han (Page 3)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/74105335250496/kyungdo-han?p=3\n",
      "ğŸ˜´ Sleeping for 9.06 seconds...\n",
      "ğŸ“„ Extracting publications for Kyungdo Han (Page 4)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/74105335250496/kyungdo-han?p=4\n",
      "ğŸ˜´ Sleeping for 12.94 seconds...\n",
      "ğŸ“„ Extracting publications for Kyungdo Han (Page 5)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/74105335250496/kyungdo-han?p=5\n",
      "ğŸ˜´ Sleeping for 5.47 seconds...\n",
      "ğŸ“„ Extracting publications for Kyungdo Han (Page 6)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/74105335250496/kyungdo-han?p=6\n",
      "ğŸ˜´ Sleeping for 9.15 seconds...\n",
      "ğŸ“„ Extracting publications for Kyungdo Han (Page 7)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/74105335250496/kyungdo-han?p=7\n",
      "ğŸ˜´ Sleeping for 5.24 seconds...\n",
      "ğŸ“„ Extracting publications for Kyungdo Han (Page 8)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/74105335250496/kyungdo-han?p=8\n",
      "ğŸ˜´ Sleeping for 7.12 seconds...\n",
      "ğŸ“„ Extracting publications for Kyungdo Han (Page 9)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/74105335250496/kyungdo-han?p=9\n",
      "ğŸ˜´ Sleeping for 8.29 seconds...\n",
      "ğŸ“„ Extracting publications for Kyungdo Han (Page 10)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/74105335250496/kyungdo-han?p=10\n",
      "ğŸ˜´ Sleeping for 5.22 seconds...\n",
      "ğŸ“„ Extracting publications for Kyungdo Han (Page 11)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/74105335250496/kyungdo-han?p=11\n",
      "ğŸ˜´ Sleeping for 6.97 seconds...\n",
      "ğŸ“„ Extracting publications for Kyungdo Han (Page 12)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/74105335250496/kyungdo-han?p=12\n",
      "ğŸ˜´ Sleeping for 11.22 seconds...\n",
      "ğŸ“„ Extracting publications for Kyungdo Han (Page 13)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/74105335250496/kyungdo-han?p=13\n",
      "ğŸ˜´ Sleeping for 6.65 seconds...\n",
      "ğŸ“„ Extracting publications for Kyungdo Han (Page 14)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/74105335250496/kyungdo-han?p=14\n",
      "ğŸ˜´ Sleeping for 11.37 seconds...\n",
      "ğŸ“„ Extracting publications for Kyungdo Han (Page 15)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/74105335250496/kyungdo-han?p=15\n",
      "ğŸ˜´ Sleeping for 9.04 seconds...\n",
      "ğŸ“„ Extracting publications for Kyungdo Han (Page 16)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/74105335250496/kyungdo-han?p=16\n",
      "ğŸ˜´ Sleeping for 9.44 seconds...\n",
      "ğŸ“„ Extracting publications for Kyungdo Han (Page 17)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/74105335250496/kyungdo-han?p=17\n",
      "ğŸ˜´ Sleeping for 14.87 seconds...\n",
      "ğŸ“„ Extracting publications for Kyungdo Han (Page 18)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/74105335250496/kyungdo-han?p=18\n",
      "ğŸ˜´ Sleeping for 12.47 seconds...\n",
      "ğŸ“„ Extracting publications for Kyungdo Han (Page 19)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/74105335250496/kyungdo-han?p=19\n",
      "ğŸ˜´ Sleeping for 11.14 seconds...\n",
      "ğŸ“„ Extracting publications for Kyungdo Han (Page 20)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/74105335250496/kyungdo-han?p=20\n",
      "ğŸ˜´ Sleeping for 9.58 seconds...\n",
      "ğŸ˜´ Sleeping for 12.96 seconds before next scholar...\n",
      "ğŸ“„ Extracting publications for Scholar 11/42: Gary S. Collins\n",
      "ğŸ“„ Extracting publications for Gary S. Collins (Page 1)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/97605358492583/gary-s-collins?p=1\n",
      "ğŸ˜´ Sleeping for 14.62 seconds...\n",
      "ğŸ“„ Extracting publications for Gary S. Collins (Page 2)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/97605358492583/gary-s-collins?p=2\n",
      "ğŸ˜´ Sleeping for 9.35 seconds...\n",
      "ğŸ“„ Extracting publications for Gary S. Collins (Page 3)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/97605358492583/gary-s-collins?p=3\n",
      "ğŸ˜´ Sleeping for 5.99 seconds...\n",
      "ğŸ“„ Extracting publications for Gary S. Collins (Page 4)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/97605358492583/gary-s-collins?p=4\n",
      "ğŸ˜´ Sleeping for 13.07 seconds...\n",
      "ğŸ“„ Extracting publications for Gary S. Collins (Page 5)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/97605358492583/gary-s-collins?p=5\n",
      "ğŸ˜´ Sleeping for 5.13 seconds...\n",
      "ğŸ“„ Extracting publications for Gary S. Collins (Page 6)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/97605358492583/gary-s-collins?p=6\n",
      "ğŸ˜´ Sleeping for 5.49 seconds...\n",
      "ğŸ“„ Extracting publications for Gary S. Collins (Page 7)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/97605358492583/gary-s-collins?p=7\n",
      "ğŸ˜´ Sleeping for 11.59 seconds...\n",
      "ğŸ“„ Extracting publications for Gary S. Collins (Page 8)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/97605358492583/gary-s-collins?p=8\n",
      "âš ï¸ Error in extract_publications_from_page: Message: \n",
      "\n",
      "ğŸš« No more publications found for Gary S. Collins on page 8. Stopping.\n",
      "ğŸ˜´ Sleeping for 28.55 seconds before next scholar...\n",
      "ğŸ›‘ Browser closed.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 179\u001b[0m\n\u001b[1;32m    176\u001b[0m     scholars_data\u001b[38;5;241m.\u001b[39mappend(current_dic)\n\u001b[1;32m    178\u001b[0m \u001b[38;5;66;03m# Crawl publications for all listed scholars\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m all_publications \u001b[38;5;241m=\u001b[39m \u001b[43mcrawl_publications_for_all_scholars\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscholars_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_pages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;66;03m# Save the extracted publication data to a CSV file\u001b[39;00m\n\u001b[1;32m    182\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(all_publications)\n",
      "Cell \u001b[0;32mIn[2], line 160\u001b[0m, in \u001b[0;36mcrawl_publications_for_all_scholars\u001b[0;34m(scholars_data, max_pages)\u001b[0m\n\u001b[1;32m    158\u001b[0m         sleep_time \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39muniform(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m30\u001b[39m)\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mğŸ˜´ Sleeping for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msleep_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds before next scholar...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 160\u001b[0m         \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43msleep_time\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    163\u001b[0m     driver\u001b[38;5;241m.\u001b[39mquit()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import re\n",
    "\n",
    "\n",
    "def init_browser():\n",
    "    \"\"\"Initialize the browser with required options and settings.\"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    chrome_options.add_argument(\"--disable-extensions\")\n",
    "    chrome_options.add_argument(\"--disable-infobars\")\n",
    "    chrome_options.add_argument(\"--disable-notifications\")\n",
    "    chrome_options.add_argument(\"--start-maximized\")\n",
    "    chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "\n",
    "    # Uncomment this line if you want to run in headless mode\n",
    "    # chrome_options.add_argument(\"--headless\")  \n",
    "\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "    driver.set_page_load_timeout(30)  # Wait up to 30 seconds for page to load\n",
    "\n",
    "    # Remove the \"navigator.webdriver\" property to avoid detection\n",
    "    driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
    "    \n",
    "    print(\"ğŸš€ Browser started successfully.\")\n",
    "    return driver\n",
    "\n",
    "\n",
    "def extract_publications_from_page(driver, scholar_profile_url, scholar_name, page_number):\n",
    "    \"\"\"Extract publications from a specific page of a scholar's profile.\"\"\"\n",
    "    try:\n",
    "        url = f\"{scholar_profile_url}?p={page_number}\"\n",
    "        print(f\"ğŸŒ Navigating to Scholar URL: {url}\")\n",
    "        driver.get(url)\n",
    "\n",
    "        # Wait until the publication entries are present on the page\n",
    "        WebDriverWait(driver, 15).until(\n",
    "            EC.presence_of_all_elements_located((By.CLASS_NAME, 'result-content'))\n",
    "        )\n",
    "\n",
    "        publications_data = []\n",
    "\n",
    "        # Find publication entries on the scholar's page\n",
    "        publication_blocks = driver.find_elements(By.CLASS_NAME, 'result-content')\n",
    "        \n",
    "        if not publication_blocks:\n",
    "            print(f\"âš ï¸ No publications found on page: {url}\")\n",
    "        \n",
    "        for block in publication_blocks:\n",
    "            try:\n",
    "                # Extract publication title and link\n",
    "                title_element = block.find_element(By.CLASS_NAME, 'publication_title').find_element(By.TAG_NAME, 'a')\n",
    "                title = title_element.text.strip() if title_element else 'N/A'\n",
    "                publication_link = title_element.get_attribute('href') if title_element else 'N/A'\n",
    "\n",
    "                # Extract publication authors\n",
    "                authors_element = block.find_element(By.CLASS_NAME, 'publication_block.authors')\n",
    "                authors = authors_element.text.strip() if authors_element else 'N/A'\n",
    "                \n",
    "                # Extract publication journal/conference name and year\n",
    "                journal_element = block.find_element(By.CLASS_NAME, 'publication_block.sub-title')\n",
    "                journal_and_year = journal_element.text.strip() if journal_element else 'N/A'\n",
    "\n",
    "                # Extract DOI link\n",
    "                try:\n",
    "                    doi_element = block.find_element(By.CLASS_NAME, 'doi_container').find_element(By.TAG_NAME, 'a')\n",
    "                    doi_link = doi_element.get_attribute('href') if doi_element else 'N/A'\n",
    "                except Exception:\n",
    "                    doi_link = 'N/A'\n",
    "\n",
    "                # Extract citation count (only the number from 'Cited by (4)')\n",
    "                try:\n",
    "                    source_block = block.find_element(By.CLASS_NAME, 'publication_block.source')\n",
    "                    cited_by_element = source_block.find_element(By.XPATH, \".//a[contains(text(), 'Cited by')]\")\n",
    "                    cited_by_text = cited_by_element.text.strip() if cited_by_element else 'N/A'\n",
    "                    # Extract only the number from 'Cited by (4)'\n",
    "                    citation_count_match = re.search(r'Cited by \\((\\d+)\\)', cited_by_text)\n",
    "                    citation_count = int(citation_count_match.group(1)) if citation_count_match else 0\n",
    "                except Exception as e:\n",
    "                    citation_count = 0\n",
    "\n",
    "                # Store the publication's data in a dictionary\n",
    "                publication_data = {\n",
    "                    'Scholar Name': scholar_name,\n",
    "                    'Title': title,\n",
    "                    'Publication Link': publication_link,\n",
    "                    'Authors': authors,\n",
    "                    'Journal/Conference': journal_and_year,\n",
    "                    'DOI': doi_link,\n",
    "                    'Citation Count': citation_count,\n",
    "                    'Scholar Profile': scholar_profile_url,\n",
    "                    'Page': page_number\n",
    "                }\n",
    "                publications_data.append(publication_data)\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Error extracting data for a publication block: {e}\")\n",
    "                continue\n",
    "\n",
    "        return publications_data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Error in extract_publications_from_page: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def crawl_publications_for_scholar(driver, scholar_name, scholar_profile_url, max_pages=5):\n",
    "    \"\"\"Crawl all pages of publications for a single scholar.\"\"\"\n",
    "    all_publications = []\n",
    "    for page_num in range(1, max_pages + 1):\n",
    "        print(f\"ğŸ“„ Extracting publications for {scholar_name} (Page {page_num})...\")\n",
    "        \n",
    "        page_publications = extract_publications_from_page(driver, scholar_profile_url, scholar_name, page_num)\n",
    "        \n",
    "        if page_publications:\n",
    "            all_publications.extend(page_publications)\n",
    "        else:\n",
    "            print(f\"ğŸš« No more publications found for {scholar_name} on page {page_num}. Stopping.\")\n",
    "            break\n",
    "\n",
    "        sleep_time = random.uniform(5, 15)\n",
    "        print(f\"ğŸ˜´ Sleeping for {sleep_time:.2f} seconds...\")\n",
    "        time.sleep(sleep_time)\n",
    "    \n",
    "    return all_publications\n",
    "\n",
    "\n",
    "def crawl_publications_for_all_scholars(scholars_data, max_pages=5):\n",
    "    \"\"\"Crawl publications for multiple scholars using their profile URLs.\"\"\"\n",
    "    driver = init_browser()\n",
    "    all_publications = []\n",
    "\n",
    "    try:\n",
    "        for index, scholar in enumerate(scholars_data):\n",
    "            print(f\"ğŸ“„ Extracting publications for Scholar {index + 1}/{len(scholars_data)}: {scholar['Name']}\")\n",
    "            scholar_name = scholar['Name']\n",
    "            scholar_profile_url = scholar['Profile Link']\n",
    "            \n",
    "            scholar_publications = crawl_publications_for_scholar(driver, scholar_name, scholar_profile_url, max_pages)\n",
    "            \n",
    "            # Save the extracted publication data to a CSV file\n",
    "            scholar_df = pd.DataFrame(scholar_publications)\n",
    "            scholar_df.to_csv(f'./stat/stat_{scholar_name}_prior5year_scholars_publications.csv', index=False)\n",
    "            \n",
    "            if scholar_publications:\n",
    "                all_publications.extend(scholar_publications)\n",
    "\n",
    "            sleep_time = random.uniform(10, 30)\n",
    "            print(f\"ğŸ˜´ Sleeping for {sleep_time:.2f} seconds before next scholar...\")\n",
    "            time.sleep(sleep_time)\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "        print(\"ğŸ›‘ Browser closed.\")\n",
    "    \n",
    "    return all_publications\n",
    "\n",
    "\n",
    "link_df = pd.read_csv('highly_ranked_scholars_2022_stat_prior5.csv')\n",
    "scholars_data = []\n",
    "for index in range(len(link_df)):\n",
    "    current_scholar = link_df.iloc[index, :]\n",
    "    current_dic = {}\n",
    "    current_dic['Name'] = current_scholar['Name']\n",
    "    current_dic['Profile Link'] = current_scholar['Profile Link']\n",
    "    scholars_data.append(current_dic)\n",
    "\n",
    "# Crawl publications for all listed scholars\n",
    "all_publications = crawl_publications_for_all_scholars(scholars_data, max_pages=20)\n",
    "\n",
    "# Save the extracted publication data to a CSV file\n",
    "df = pd.DataFrame(all_publications)\n",
    "df.to_csv('stat_prior5year_scholars_publications_2.csv', index=False)\n",
    "\n",
    "print(f\"âœ… Data successfully saved to stat_prior5year_scholars_publications_2.csv. Total publications extracted: {len(all_publications)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### continue from Gary S. Collins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guiwu Wei\n",
      "Mike Thelwall\n",
      "Andrew Gelman\n",
      "Malin Song\n",
      "Daniel Rueckert\n",
      "Jianzhou Wang\n",
      "Muhammad Aslam\n",
      "Deyu Meng\n",
      "Douglas G. Altman\n",
      "Kyungdo Han\n",
      "Gary S. Collins\n",
      "Johann S. De Bono\n",
      "Serge Hercberg\n",
      "Trevor Hastie\n",
      "Massimo Ciccozzi\n",
      "Rob J. Hyndman\n",
      "Andreas Holzinger\n",
      "Peng Ding\n",
      "Yong Du\n",
      "Naomi Altman\n",
      "Giovanni Sotgiu\n",
      "Gerta RÃ¼cker\n",
      "Martin J. Wainwright\n",
      "Satya N. Majumdar\n",
      "Dylan S. Small\n",
      "Martin Krzywinski\n",
      "Adam M. Phillippy\n",
      "Abdul Haq\n",
      "David B. Dunson\n",
      "Stuart J. Pocock\n",
      "Giovanni Corrao\n",
      "Bin Xu\n",
      "Marcel A. L. M. Van Assen\n",
      "Antonio Gasparrini\n",
      "Yvan Saeys\n",
      "Maxim Finkelstein\n",
      "Umapada Pal\n",
      "Anna Chaimani\n",
      "JosÃ© Crossa\n",
      "Sergey Koren\n",
      "Jiu-Ying Dong\n",
      "Jun Cheng\n",
      "ğŸš€ Browser started successfully.\n",
      "ğŸ“„ Extracting publications for Scholar 1/32: Gary S. Collins\n",
      "ğŸ“„ Extracting publications for Gary S. Collins (Page 1)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/97605358492583/gary-s-collins?p=1\n",
      "âš ï¸ Error in extract_publications_from_page: Message: no such window: target window already closed\n",
      "from unknown error: web view not found\n",
      "  (Session info: chrome=128.0.6613.85)\n",
      "Stacktrace:\n",
      "0   chromedriver                        0x0000000100925208 cxxbridge1$str$ptr + 1927396\n",
      "1   chromedriver                        0x000000010091d66c cxxbridge1$str$ptr + 1895752\n",
      "2   chromedriver                        0x0000000100518808 cxxbridge1$string$len + 89564\n",
      "3   chromedriver                        0x00000001004f3b0c core::str::slice_error_fail::h6c488016ada29016 + 3776\n",
      "4   chromedriver                        0x00000001005834d8 cxxbridge1$string$len + 527020\n",
      "5   chromedriver                        0x0000000100595c90 cxxbridge1$string$len + 602724\n",
      "6   chromedriver                        0x0000000100551698 cxxbridge1$string$len + 322668\n",
      "7   chromedriver                        0x0000000100552310 cxxbridge1$string$len + 325860\n",
      "8   chromedriver                        0x00000001008ebe78 cxxbridge1$str$ptr + 1693012\n",
      "9   chromedriver                        0x00000001008f077c cxxbridge1$str$ptr + 1711704\n",
      "10  chromedriver                        0x00000001008d13ec cxxbridge1$str$ptr + 1583816\n",
      "11  chromedriver                        0x00000001008f104c cxxbridge1$str$ptr + 1713960\n",
      "12  chromedriver                        0x00000001008c1fc8 cxxbridge1$str$ptr + 1521316\n",
      "13  chromedriver                        0x000000010090eb68 cxxbridge1$str$ptr + 1835588\n",
      "14  chromedriver                        0x000000010090ece4 cxxbridge1$str$ptr + 1835968\n",
      "15  chromedriver                        0x000000010091d308 cxxbridge1$str$ptr + 1894884\n",
      "16  libsystem_pthread.dylib             0x00000001823132e4 _pthread_start + 136\n",
      "17  libsystem_pthread.dylib             0x000000018230e0fc thread_start + 8\n",
      "\n",
      "ğŸš« No more publications found for Gary S. Collins on page 1. Stopping.\n",
      "ğŸ˜´ Sleeping for 10.22 seconds before next scholar...\n",
      "ğŸ›‘ Browser closed.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m     scholars_data\u001b[38;5;241m.\u001b[39mappend(current_dic)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Crawl publications for all listed scholars\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m all_publications \u001b[38;5;241m=\u001b[39m \u001b[43mcrawl_publications_for_all_scholars\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscholars_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_pages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Save the extracted publication data to a CSV file\u001b[39;00m\n\u001b[1;32m     19\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(all_publications)\n",
      "Cell \u001b[0;32mIn[2], line 160\u001b[0m, in \u001b[0;36mcrawl_publications_for_all_scholars\u001b[0;34m(scholars_data, max_pages)\u001b[0m\n\u001b[1;32m    158\u001b[0m         sleep_time \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39muniform(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m30\u001b[39m)\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mğŸ˜´ Sleeping for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msleep_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds before next scholar...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 160\u001b[0m         \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43msleep_time\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    163\u001b[0m     driver\u001b[38;5;241m.\u001b[39mquit()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "scholars_crawled = ['Guiwu Wei', 'Mike Thelwall', 'Andrew Gelman', 'Malin Song', 'Daniel Rueckert', 'Jianzhou Wang', 'Muhammad Aslam', 'Deyu Meng', 'Douglas G. Altman', 'Kyungdo Han']\n",
    "link_df = pd.read_csv('highly_ranked_scholars_2022_stat_prior5.csv')\n",
    "scholars_data = []\n",
    "for index in range(len(link_df)):\n",
    "    current_scholar = link_df.iloc[index, :]\n",
    "    print(current_scholar['Name'])\n",
    "    if current_scholar['Name'] in scholars_crawled:\n",
    "        continue\n",
    "    \n",
    "    current_dic = {}\n",
    "    current_dic['Name'] = current_scholar['Name']\n",
    "    current_dic['Profile Link'] = current_scholar['Profile Link']\n",
    "    scholars_data.append(current_dic)\n",
    "\n",
    "# Crawl publications for all listed scholars\n",
    "all_publications = crawl_publications_for_all_scholars(scholars_data, max_pages=20)\n",
    "\n",
    "# Save the extracted publication data to a CSV file\n",
    "df = pd.DataFrame(all_publications)\n",
    "df.to_csv('stat_prior5year_scholars_publications_2.csv', index=False)\n",
    "\n",
    "print(f\"âœ… Data successfully saved to stat_prior5year_scholars_publications_2.csv. Total publications extracted: {len(all_publications)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### test: using proxy id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“„ Extracting publications for Scholar 1/42: Guiwu Wei\n",
      "ğŸ”„ Switching to new proxy: http://34.100.138.252:8660\n",
      "ğŸš€ Browser started successfully.\n",
      "ğŸ“„ Extracting publications for Guiwu Wei (Page 1)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/78021622765809/guiwu-wei?p=1\n",
      "âš ï¸ Rate limit or CAPTCHA detected! Retrying after a long wait...\n",
      "âš ï¸ Rate limit detected! Sleeping for 288.87 seconds...\n",
      "ğŸ”„ Switching to new proxy: http://103.152.112.120:80\n",
      "ğŸš€ Browser started successfully.\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/78021622765809/guiwu-wei?p=1\n",
      "âš ï¸ Error in extract_publications_from_page: Message: unknown error: net::ERR_TUNNEL_CONNECTION_FAILED\n",
      "  (Session info: chrome=128.0.6613.85)\n",
      "Stacktrace:\n",
      "0   chromedriver                        0x000000010483d208 cxxbridge1$str$ptr + 1927396\n",
      "1   chromedriver                        0x000000010483566c cxxbridge1$str$ptr + 1895752\n",
      "2   chromedriver                        0x0000000104430808 cxxbridge1$string$len + 89564\n",
      "3   chromedriver                        0x0000000104429cf0 cxxbridge1$string$len + 62148\n",
      "4   chromedriver                        0x000000010441bbd8 cxxbridge1$string$len + 4524\n",
      "5   chromedriver                        0x000000010441d628 cxxbridge1$string$len + 11260\n",
      "6   chromedriver                        0x000000010441bf8c cxxbridge1$string$len + 5472\n",
      "7   chromedriver                        0x000000010441b800 cxxbridge1$string$len + 3540\n",
      "8   chromedriver                        0x000000010441b74c cxxbridge1$string$len + 3360\n",
      "9   chromedriver                        0x0000000104419ae8 core::str::slice_error_fail::h6c488016ada29016 + 61084\n",
      "10  chromedriver                        0x000000010441a058 core::str::slice_error_fail::h6c488016ada29016 + 62476\n",
      "11  chromedriver                        0x0000000104432cb8 cxxbridge1$string$len + 98956\n",
      "12  chromedriver                        0x00000001044aebb8 cxxbridge1$string$len + 606604\n",
      "13  chromedriver                        0x00000001044ae228 cxxbridge1$string$len + 604156\n",
      "14  chromedriver                        0x0000000104469698 cxxbridge1$string$len + 322668\n",
      "15  chromedriver                        0x000000010446a310 cxxbridge1$string$len + 325860\n",
      "16  chromedriver                        0x0000000104803e78 cxxbridge1$str$ptr + 1693012\n",
      "17  chromedriver                        0x000000010480877c cxxbridge1$str$ptr + 1711704\n",
      "18  chromedriver                        0x00000001047e93ec cxxbridge1$str$ptr + 1583816\n",
      "19  chromedriver                        0x000000010480904c cxxbridge1$str$ptr + 1713960\n",
      "20  chromedriver                        0x00000001047d9fc8 cxxbridge1$str$ptr + 1521316\n",
      "21  chromedriver                        0x0000000104826b68 cxxbridge1$str$ptr + 1835588\n",
      "22  chromedriver                        0x0000000104826ce4 cxxbridge1$str$ptr + 1835968\n",
      "23  chromedriver                        0x0000000104835308 cxxbridge1$str$ptr + 1894884\n",
      "24  libsystem_pthread.dylib             0x000000018a4832e4 _pthread_start + 136\n",
      "25  libsystem_pthread.dylib             0x000000018a47e0fc thread_start + 8\n",
      "\n",
      "ğŸš« No more publications found for Guiwu Wei on page 1. Stopping.\n",
      "ğŸ›‘ Browser closed.\n",
      "ğŸ“„ Extracting publications for Scholar 2/42: Mike Thelwall\n",
      "ğŸ”„ Switching to new proxy: http://34.93.180.113:8660\n",
      "ğŸš€ Browser started successfully.\n",
      "ğŸ“„ Extracting publications for Mike Thelwall (Page 1)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/37192170410417/mike-thelwall?p=1\n",
      "âš ï¸ Rate limit or CAPTCHA detected! Retrying after a long wait...\n",
      "âš ï¸ Rate limit detected! Sleeping for 219.60 seconds...\n",
      "ğŸ”„ Switching to new proxy: http://34.100.138.252:8660\n",
      "ğŸš€ Browser started successfully.\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/37192170410417/mike-thelwall?p=1\n",
      "âš ï¸ Rate limit or CAPTCHA detected! Retrying after a long wait...\n",
      "ğŸ“„ Extracting publications for Mike Thelwall (Page 2)...\n",
      "ğŸŒ Navigating to Scholar URL: https://scholargps.com/scholars/37192170410417/mike-thelwall?p=2\n",
      "âš ï¸ Rate limit or CAPTCHA detected! Retrying after a long wait...\n",
      "âš ï¸ Rate limit detected! Sleeping for 126.47 seconds...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 168\u001b[0m\n\u001b[1;32m    165\u001b[0m scholars_data \u001b[38;5;241m=\u001b[39m link_df\u001b[38;5;241m.\u001b[39mto_dict(orient\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecords\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    167\u001b[0m \u001b[38;5;66;03m# Crawl publications for all listed scholars\u001b[39;00m\n\u001b[0;32m--> 168\u001b[0m all_publications \u001b[38;5;241m=\u001b[39m \u001b[43mcrawl_publications_for_all_scholars\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscholars_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_pages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# Save the extracted publication data to a CSV file\u001b[39;00m\n\u001b[1;32m    171\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(all_publications)\n",
      "Cell \u001b[0;32mIn[18], line 153\u001b[0m, in \u001b[0;36mcrawl_publications_for_all_scholars\u001b[0;34m(scholars_data, max_pages)\u001b[0m\n\u001b[1;32m    150\u001b[0m scholar_profile_url \u001b[38;5;241m=\u001b[39m scholar[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProfile Link\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mğŸ“„ Extracting publications for Scholar \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindex\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(scholars_data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscholar_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 153\u001b[0m scholar_publications \u001b[38;5;241m=\u001b[39m \u001b[43mcrawl_publications_for_scholar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscholar_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscholar_profile_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_pages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m scholar_publications:\n\u001b[1;32m    156\u001b[0m     all_publications\u001b[38;5;241m.\u001b[39mextend(scholar_publications)\n",
      "Cell \u001b[0;32mIn[18], line 120\u001b[0m, in \u001b[0;36mcrawl_publications_for_scholar\u001b[0;34m(scholar_name, scholar_profile_url, max_pages)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrate_limit\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    119\u001b[0m     driver\u001b[38;5;241m.\u001b[39mquit()\n\u001b[0;32m--> 120\u001b[0m     \u001b[43mhandle_rate_limit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m     proxy \u001b[38;5;241m=\u001b[39m get_random_proxy()\n\u001b[1;32m    122\u001b[0m     driver \u001b[38;5;241m=\u001b[39m init_browser(proxy)\n",
      "Cell \u001b[0;32mIn[18], line 102\u001b[0m, in \u001b[0;36mhandle_rate_limit\u001b[0;34m()\u001b[0m\n\u001b[1;32m    100\u001b[0m wait_time \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39muniform(\u001b[38;5;241m120\u001b[39m, \u001b[38;5;241m300\u001b[39m)  \u001b[38;5;66;03m# Wait 2 to 5 minutes\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâš ï¸ Rate limit detected! Sleeping for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwait_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 102\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait_time\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import re\n",
    "\n",
    "\n",
    "# List of Proxies (replace with valid proxies)\n",
    "PROXY_LIST = [\n",
    "    'http://103.152.112.120:80',\n",
    "    'http://34.93.180.113:8660',\n",
    "    'http://34.100.138.252:8660',\n",
    "    'http://103.56.206.65:4995',\n",
    "    'http://35.215.216.90:80'\n",
    "]\n",
    "\n",
    "\n",
    "def get_random_proxy():\n",
    "    \"\"\"Return a random proxy from the list.\"\"\"\n",
    "    proxy = random.choice(PROXY_LIST)\n",
    "    print(f\"ğŸ”„ Switching to new proxy: {proxy}\")\n",
    "    return proxy\n",
    "\n",
    "\n",
    "def init_browser(proxy=None):\n",
    "    \"\"\"Initialize the browser with proxy and required options.\"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    chrome_options.add_argument(\"--disable-extensions\")\n",
    "    chrome_options.add_argument(\"--disable-infobars\")\n",
    "    chrome_options.add_argument(\"--disable-notifications\")\n",
    "    chrome_options.add_argument(\"--start-maximized\")\n",
    "    chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "\n",
    "    if proxy:\n",
    "        chrome_options.add_argument(f'--proxy-server={proxy}')\n",
    "\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "    driver.set_page_load_timeout(30)\n",
    "\n",
    "    # Remove the \"navigator.webdriver\" property to avoid detection\n",
    "    driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
    "    \n",
    "    print(\"ğŸš€ Browser started successfully.\")\n",
    "    return driver\n",
    "\n",
    "\n",
    "def extract_publications_from_page(driver, scholar_profile_url, scholar_name, page_number):\n",
    "    \"\"\"Extract publications from a specific page of a scholar's profile.\"\"\"\n",
    "    try:\n",
    "        url = f\"{scholar_profile_url}?p={page_number}\"\n",
    "        print(f\"ğŸŒ Navigating to Scholar URL: {url}\")\n",
    "        driver.get(url)\n",
    "\n",
    "        # Check for CAPTCHA or rate limit\n",
    "        if \"rate limit\" in driver.page_source.lower() or \"too many requests\" in driver.page_source.lower() or \"captcha\" in driver.page_source.lower():\n",
    "            print(\"âš ï¸ Rate limit or CAPTCHA detected! Retrying after a long wait...\")\n",
    "            return \"rate_limit\"\n",
    "\n",
    "        # Wait until the publication entries are present on the page\n",
    "        WebDriverWait(driver, 15).until(\n",
    "            EC.presence_of_all_elements_located((By.CLASS_NAME, 'result-content'))\n",
    "        )\n",
    "\n",
    "        publications_data = []\n",
    "        publication_blocks = driver.find_elements(By.CLASS_NAME, 'result-content')\n",
    "        \n",
    "        for block in publication_blocks:\n",
    "            try:\n",
    "                title_element = block.find_element(By.CLASS_NAME, 'publication_title').find_element(By.TAG_NAME, 'a')\n",
    "                title = title_element.text.strip()\n",
    "                publication_link = title_element.get_attribute('href')\n",
    "                publication_data = {\n",
    "                    'Scholar Name': scholar_name,\n",
    "                    'Title': title,\n",
    "                    'Publication Link': publication_link\n",
    "                }\n",
    "                publications_data.append(publication_data)\n",
    "            except Exception as e:\n",
    "                continue\n",
    "\n",
    "        return publications_data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Error in extract_publications_from_page: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def handle_rate_limit():\n",
    "    \"\"\"Handle rate limits by waiting and switching proxy.\"\"\"\n",
    "    wait_time = random.uniform(120, 300)  # Wait 2 to 5 minutes\n",
    "    print(f\"âš ï¸ Rate limit detected! Sleeping for {wait_time:.2f} seconds...\")\n",
    "    time.sleep(wait_time)\n",
    "\n",
    "\n",
    "def crawl_publications_for_scholar(scholar_name, scholar_profile_url, max_pages=5):\n",
    "    \"\"\"Crawl all pages of publications for a single scholar.\"\"\"\n",
    "    all_publications = []\n",
    "    driver = None\n",
    "    for page_num in range(1, max_pages + 1):\n",
    "        try:\n",
    "            if not driver:\n",
    "                proxy = get_random_proxy()\n",
    "                driver = init_browser(proxy)\n",
    "\n",
    "            print(f\"ğŸ“„ Extracting publications for {scholar_name} (Page {page_num})...\")\n",
    "            result = extract_publications_from_page(driver, scholar_profile_url, scholar_name, page_num)\n",
    "            \n",
    "            if result == \"rate_limit\":\n",
    "                driver.quit()\n",
    "                handle_rate_limit()\n",
    "                proxy = get_random_proxy()\n",
    "                driver = init_browser(proxy)\n",
    "                result = extract_publications_from_page(driver, scholar_profile_url, scholar_name, page_num)\n",
    "            \n",
    "            if result:\n",
    "                all_publications.extend(result)\n",
    "            else:\n",
    "                print(f\"ğŸš« No more publications found for {scholar_name} on page {page_num}. Stopping.\")\n",
    "                break\n",
    "\n",
    "            time.sleep(random.uniform(5, 15))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error for scholar {scholar_name} on page {page_num}: {e}\")\n",
    "            break\n",
    "\n",
    "    if driver:\n",
    "        driver.quit()\n",
    "        print(\"ğŸ›‘ Browser closed.\")\n",
    "    \n",
    "    return all_publications\n",
    "\n",
    "\n",
    "def crawl_publications_for_all_scholars(scholars_data, max_pages=5):\n",
    "    \"\"\"Crawl publications for multiple scholars using their profile URLs.\"\"\"\n",
    "    all_publications = []\n",
    "\n",
    "    for index, scholar in enumerate(scholars_data):\n",
    "        scholar_name = scholar['Name']\n",
    "        scholar_profile_url = scholar['Profile Link']\n",
    "        print(f\"ğŸ“„ Extracting publications for Scholar {index + 1}/{len(scholars_data)}: {scholar_name}\")\n",
    "        \n",
    "        scholar_publications = crawl_publications_for_scholar(scholar_name, scholar_profile_url, max_pages)\n",
    "        \n",
    "        if scholar_publications:\n",
    "            all_publications.extend(scholar_publications)\n",
    "\n",
    "        time.sleep(random.uniform(10, 30))\n",
    "    \n",
    "    return all_publications\n",
    "\n",
    "\n",
    "# Read scholar list from CSV file\n",
    "link_df = pd.read_csv('highly_ranked_scholars_2022_stat_prior5.csv')\n",
    "scholars_data = link_df.to_dict(orient='records')\n",
    "\n",
    "# Crawl publications for all listed scholars\n",
    "all_publications = crawl_publications_for_all_scholars(scholars_data, max_pages=20)\n",
    "\n",
    "# Save the extracted publication data to a CSV file\n",
    "df = pd.DataFrame(all_publications)\n",
    "df.to_csv('stat_prior5year_scholars_publications_2.csv', index=False)\n",
    "\n",
    "print(f\"âœ… Data successfully saved to stat_prior5year_scholars_publications_2.csv. Total publications extracted: {len(all_publications)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scholar_scraper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
